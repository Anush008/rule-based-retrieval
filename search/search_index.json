{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Rule-based Retrieval Documentation","text":"<p>The Rule-based Retrieval package is a Python package for creating Retrieval Augmented Generation (RAG) applications with filtering capabilities. It leverages OpenAI for text generation and Pinecone for vector database management.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Easy-to-use API for creating and managing Pinecone indexes</li> <li>Uploading and processing documents (currently supports PDF files)</li> <li>Generating embeddings using OpenAI models</li> <li>Querying the index with custom filtering rules</li> <li>Retrieval Augmented Generation for question answering</li> <li>Querying the index with custom filtering rules, including processing rules separately and triggering rules based on keywords</li> </ul>"},{"location":"#getting-started","title":"Getting Started","text":"<ol> <li>Install the package by following the Installation Guide</li> <li>Set up your OpenAI and Pinecone API keys as environment variables</li> <li>Create an index and upload your documents using the <code>Client</code> class</li> <li>Query the index with custom rules to retrieve relevant documents, optionally processing rules separately or triggering rules based on keywords</li> <li>Use the retrieved documents to generate answers to your questions</li> </ol> <p>For a detailed walkthrough and code examples, check out the Tutorial.</p>"},{"location":"#architecture-overview","title":"Architecture Overview","text":"<p>The Rule-based Retrieval package consists of the following main components:</p> <ul> <li><code>Client</code>: The central class for managing resources and performing RAG-related tasks</li> <li><code>Rule</code>: Allows defining custom filtering rules for retrieving documents</li> <li><code>PineconeMetadata</code> and <code>PineconeDocument</code>: Classes for representing and storing document metadata and embeddings in Pinecone</li> <li><code>embedding</code>, <code>processing</code>, and <code>exceptions</code> modules: Utility functions and custom exceptions</li> </ul>"},{"location":"api/","title":"Reference","text":""},{"location":"api/#whyhowembedding-module","title":"<code>whyhow.embedding</code> module","text":""},{"location":"api/#whyhow.embedding.generate_embeddings","title":"<code>whyhow.embedding.generate_embeddings(openai_api_key, chunks, model='text-embedding-ada-002')</code>","text":"<p>Generate embeddings for a list of chunks.</p> Source code in <code>whyhow/embedding.py</code> <pre><code>def generate_embeddings(\n    openai_api_key: str,\n    chunks: list[str],\n    model: str = \"text-embedding-ada-002\",\n) -&gt; list[list[float]]:\n    \"\"\"Generate embeddings for a list of chunks.\"\"\"\n    embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key, model=model)  # type: ignore[call-arg]\n    embeddings_array = embeddings.embed_documents(chunks)\n\n    return embeddings_array\n</code></pre>"},{"location":"api/#whyhowexceptions-module","title":"<code>whyhow.exceptions</code> module","text":""},{"location":"api/#whyhow.exceptions.IndexAlreadyExistsException","title":"<code>whyhow.exceptions.IndexAlreadyExistsException</code>","text":"<p>             Bases: <code>Exception</code></p> <p>Raised when the index already exists.</p> Source code in <code>whyhow/exceptions.py</code> <pre><code>class IndexAlreadyExistsException(Exception):\n    \"\"\"Raised when the index already exists.\"\"\"\n\n    pass\n</code></pre>"},{"location":"api/#whyhow.exceptions.IndexNotFoundException","title":"<code>whyhow.exceptions.IndexNotFoundException</code>","text":"<p>             Bases: <code>Exception</code></p> <p>Raised when the index is not found.</p> Source code in <code>whyhow/exceptions.py</code> <pre><code>class IndexNotFoundException(Exception):\n    \"\"\"Raised when the index is not found.\"\"\"\n\n    pass\n</code></pre>"},{"location":"api/#whyhow.exceptions.OpenAIException","title":"<code>whyhow.exceptions.OpenAIException</code>","text":"<p>             Bases: <code>Exception</code></p> <p>Raised when the OpenAI API returns an error.</p> Source code in <code>whyhow/exceptions.py</code> <pre><code>class OpenAIException(Exception):\n    \"\"\"Raised when the OpenAI API returns an error.\"\"\"\n\n    pass\n</code></pre>"},{"location":"api/#whyhowprocessing-module","title":"<code>whyhow.processing</code> module","text":""},{"location":"api/#whyhow.processing.parse_and_split","title":"<code>whyhow.processing.parse_and_split(path, chunk_size=512, chunk_overlap=100)</code>","text":"<p>Parse a PDF and split it into chunks.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str or Path</code> <p>Path to the document to process.</p> required <p>Returns:</p> Type Description <code>list[Document]</code> <p>The chunks of the pdf.</p> Source code in <code>whyhow/processing.py</code> <pre><code>def parse_and_split(\n    path: str | pathlib.Path,\n    chunk_size: int = 512,\n    chunk_overlap: int = 100,\n) -&gt; list[Document]:\n    \"\"\"Parse a PDF and split it into chunks.\n\n    Parameters\n    ----------\n    path : str or pathlib.Path\n        Path to the document to process.\n\n    Returns\n    -------\n    list[Document]\n        The chunks of the pdf.\n    \"\"\"\n    loader = PyPDFLoader(str(path))\n    docs = loader.load()\n    splitter = RecursiveCharacterTextSplitter(\n        chunk_size=chunk_size,\n        chunk_overlap=chunk_overlap,\n    )\n    chunks = splitter.split_documents(docs)\n\n    # Assign the change number (within a page) to each chunk\n    i_page = 0\n    i_chunk = 0\n\n    for chunk in chunks:\n        if chunk.metadata[\"page\"] != i_page:\n            i_page = chunk.metadata[\"page\"]\n            i_chunk = 0\n\n        chunk.metadata[\"chunk\"] = i_chunk\n        i_chunk += 1\n\n    return chunks\n</code></pre>"},{"location":"api/#whyhow.processing.clean_chunks","title":"<code>whyhow.processing.clean_chunks(chunks)</code>","text":"<p>Clean the chunks of a pdf.</p> <p>No modifications in-place.</p> <p>Parameters:</p> Name Type Description Default <code>chunks</code> <code>list[Document]</code> <p>The chunks of the pdf.</p> required <p>Returns:</p> Type Description <code>list[Document]</code> <p>The cleaned chunks.</p> Source code in <code>whyhow/processing.py</code> <pre><code>def clean_chunks(\n    chunks: list[Document],\n) -&gt; list[Document]:\n    \"\"\"Clean the chunks of a pdf.\n\n    No modifications in-place.\n\n    Parameters\n    ----------\n    chunks : list[Document]\n        The chunks of the pdf.\n\n    Returns\n    -------\n    list[Document]\n        The cleaned chunks.\n    \"\"\"\n    pattern = re.compile(r\"(\\r\\n|\\n|\\r)\")\n    clean_chunks: list[Document] = []\n\n    for chunk in chunks:\n        text = re.sub(pattern, \"\", chunk.page_content)\n        new_chunk = Document(\n            page_content=text,\n            metadata=copy.deepcopy(chunk.metadata),\n        )\n\n        clean_chunks.append(new_chunk)\n\n    return clean_chunks\n</code></pre>"},{"location":"api/#whyhowrag-module","title":"<code>whyhow.rag</code> module","text":""},{"location":"api/#whyhow.rag.Client","title":"<code>whyhow.rag.Client</code>","text":"<p>Synchronous client.</p> Source code in <code>whyhow/rag.py</code> <pre><code>class Client:\n    \"\"\"Synchronous client.\"\"\"\n\n    def __init__(\n        self,\n        openai_api_key: str | None = None,\n        pinecone_api_key: str | None = None,\n    ):\n        if openai_api_key is None:\n            openai_api_key = os.environ.get(\"OPENAI_API_KEY\")\n            if openai_api_key is None:\n                raise ValueError(\n                    \"No OPENAI_API_KEY provided must be provided.\"\n                )\n\n        if pinecone_api_key is None:\n            pinecone_api_key = os.environ.get(\"PINECONE_API_KEY\")\n            if pinecone_api_key is None:\n                raise ValueError(\"No PINECONE_API_KEY provided\")\n\n        self.openai_client = OpenAI(api_key=openai_api_key)\n        self.pinecone_client = Pinecone(api_key=pinecone_api_key)\n\n    def get_index(self, name: str) -&gt; Index:\n        \"\"\"Get an existing index.\n\n        Parameters\n        ----------\n        name : str\n            The name of the index.\n\n\n        Returns\n        -------\n        Index\n            The index.\n\n        Raises\n        ------\n        ValueError\n            If the index does not exist.\n\n        \"\"\"\n        try:\n            index = self.pinecone_client.Index(name)\n        except NotFoundException as e:\n            raise IndexNotFoundException(f\"Index {name} does not exist\") from e\n\n        return index\n\n    def create_index(\n        self,\n        name: str,\n        dimension: int = 1536,\n        metric: Metric = \"cosine\",\n        spec: ServerlessSpec | PodSpec | None = None,\n    ) -&gt; Index:\n        \"\"\"Create a new index.\n\n        If the index does not exist, it creates a new index with the specified.\n\n        Parameters\n        ----------\n        name : str\n            The name of the index.\n\n        dimension : int\n            The dimension of the index.\n\n        metric : Metric\n            The metric of the index.\n\n        spec : ServerlessSpec | PodSpec | None\n            The spec of the index. If None, it uses the default spec.\n\n        Raises\n        ------\n        ValueError\n            If the index already exists.\n\n        \"\"\"\n        try:\n            self.get_index(name)\n        except IndexNotFoundException:\n            pass\n        else:\n            raise IndexAlreadyExistsException(f\"Index {name} already exists\")\n\n        if spec is None:\n            spec = DEFAULT_SPEC\n            logging.info(f\"Using default spec {spec}\")\n\n        self.pinecone_client.create_index(\n            name=name, dimension=dimension, metric=metric, spec=spec\n        )\n        index = self.pinecone_client.Index(name)\n\n        return index\n\n    def upload_documents(\n        self,\n        index: Index,\n        documents: list[str | pathlib.Path],\n        namespace: str,\n        embedding_model: str = \"text-embedding-ada-002\",\n        batch_size: int = 100,\n    ) -&gt; None:\n        \"\"\"Upload documents to the index.\n\n        Parameters\n        ----------\n        index : Index\n            The index.\n\n        documents : list[str | pathlib.Path]\n            The documents to upload.\n\n        namespace : str\n            The namespace within the index to use.\n\n        batch_size : int\n            The number of documents to upload at a time.\n\n        embedding_model : str\n            The OpenAI embedding model to use.\n\n        \"\"\"\n        # don't allow for duplicate documents\n        documents = list(set(documents))\n        if not documents:\n            logger.info(\"No documents to upload\")\n            return\n\n        logger.info(f\"Parsing {len(documents)} documents\")\n        all_chunks: list[Document] = []\n        for document in documents:\n            chunks_ = parse_and_split(document)\n            chunks = clean_chunks(chunks_)\n            all_chunks.extend(chunks)\n\n        logger.info(f\"Embedding {len(all_chunks)} chunks\")\n        embeddings = generate_embeddings(\n            openai_api_key=self.openai_client.api_key,\n            chunks=[c.page_content for c in all_chunks],\n            model=embedding_model,\n        )\n\n        if len(embeddings) != len(all_chunks):\n            raise ValueError(\n                \"Number of embeddings does not match number of chunks\"\n            )\n\n        # create PineconeDocuments\n        pinecone_documents = []\n        for i, (chunk, embedding) in enumerate(zip(all_chunks, embeddings)):\n            metadata = PineconeMetadata(\n                text=chunk.page_content,\n                page_number=chunk.metadata[\"page\"],\n                chunk_number=chunk.metadata[\"chunk\"],\n                filename=chunk.metadata[\"source\"],\n            )\n            pinecone_document = PineconeDocument(\n                values=embedding,\n                metadata=metadata,\n            )\n            pinecone_documents.append(pinecone_document)\n\n        upsert_documents = [d.model_dump() for d in pinecone_documents]\n\n        response = index.upsert(\n            upsert_documents, namespace=namespace, batch_size=batch_size\n        )\n        n_upserted = response[\"upserted_count\"]\n        logger.info(f\"Upserted {n_upserted} documents\")\n\n    def query(\n        self,\n        question: str,\n        index: Index,\n        namespace: str,\n        rules: list[Rule] | None = None,\n        top_k: int = 5,\n        chat_model: str = \"gpt-3.5-turbo\",\n        chat_temperature: float = 0.0,\n        chat_max_tokens: int = 1000,\n        chat_seed: int = 2,\n        embedding_model: str = \"text-embedding-ada-002\",\n    ) -&gt; QueryReturnType:\n        \"\"\"Query the index.\"\"\"\n        if rules is None:\n            rules = []\n\n        filters = [rule.to_filter() for rule in rules]\n        filters = [f for f in filters if f is not None]\n        logger.info(f\"Using {len(filters)} filters (from {len(rules)} rules)\")\n        if not filters:\n            filter = None\n        else:\n            filter = {\"$or\": [f for f in filters]}\n\n        question_embedding = generate_embeddings(\n            openai_api_key=self.openai_client.api_key,\n            chunks=[question],\n            model=embedding_model,\n        )[0]\n\n        query_response = index.query(\n            namespace=namespace,\n            top_k=top_k,\n            vector=question_embedding,\n            filter=filter,\n            include_metadata=True,\n        )\n        matches = [\n            PineconeMatch(**m.to_dict()) for m in query_response[\"matches\"]\n        ]\n\n        texts = [m.metadata.text for m in matches]\n        input_actual = Input(question=question, contexts=texts)\n\n        prompt_end = f\"\"\"\nACTUAL INPUT\n```json\n{input_actual.model_dump_json()}\n```\n\nACTUAL OUTPUT\n\"\"\"\n        prompt = f\"{PROMPT_START}\\n{prompt_end}\"\n        logger.debug(f\"Prompt: {prompt}\")\n\n        response = self.openai_client.chat.completions.create(\n            model=chat_model,\n            seed=chat_seed,\n            temperature=chat_temperature,\n            messages=[{\"role\": \"user\", \"content\": prompt}],\n            max_tokens=chat_max_tokens,\n        )\n\n        choice = response.choices[0]\n        if choice.finish_reason != \"stop\":\n            raise OpenAIException(\n                f\"Chat did not finish. Reason: {choice.finish_reason}\"\n            )\n\n        response_raw = cast(str, response.choices[0].message.content)\n\n        if response_raw.startswith(\"```json\"):\n            start_i = response_raw.index(\"{\")\n            end_i = response_raw.rindex(\"}\")\n            response_raw = response_raw[start_i : end_i + 1]\n\n        try:\n            output = Output.model_validate_json(response_raw)\n        except ValidationError as e:\n            raise OpenAIException(\n                f\"OpenAI did not return a valid JSON: {response_raw}\"\n            ) from e\n\n        return_dict: QueryReturnType = {\n            \"answer\": output.answer,\n            \"matches\": [m.model_dump() for m in matches],\n            \"used_contexts\": output.contexts,\n        }\n\n        return return_dict\n</code></pre>"},{"location":"api/#whyhow.rag.Client.create_index","title":"<code>create_index(name, dimension=1536, metric='cosine', spec=None)</code>","text":"<p>Create a new index.</p> <p>If the index does not exist, it creates a new index with the specified.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the index.</p> required <code>dimension</code> <code>int</code> <p>The dimension of the index.</p> <code>1536</code> <code>metric</code> <code>Metric</code> <p>The metric of the index.</p> <code>'cosine'</code> <code>spec</code> <code>ServerlessSpec | PodSpec | None</code> <p>The spec of the index. If None, it uses the default spec.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the index already exists.</p> Source code in <code>whyhow/rag.py</code> <pre><code>def create_index(\n    self,\n    name: str,\n    dimension: int = 1536,\n    metric: Metric = \"cosine\",\n    spec: ServerlessSpec | PodSpec | None = None,\n) -&gt; Index:\n    \"\"\"Create a new index.\n\n    If the index does not exist, it creates a new index with the specified.\n\n    Parameters\n    ----------\n    name : str\n        The name of the index.\n\n    dimension : int\n        The dimension of the index.\n\n    metric : Metric\n        The metric of the index.\n\n    spec : ServerlessSpec | PodSpec | None\n        The spec of the index. If None, it uses the default spec.\n\n    Raises\n    ------\n    ValueError\n        If the index already exists.\n\n    \"\"\"\n    try:\n        self.get_index(name)\n    except IndexNotFoundException:\n        pass\n    else:\n        raise IndexAlreadyExistsException(f\"Index {name} already exists\")\n\n    if spec is None:\n        spec = DEFAULT_SPEC\n        logging.info(f\"Using default spec {spec}\")\n\n    self.pinecone_client.create_index(\n        name=name, dimension=dimension, metric=metric, spec=spec\n    )\n    index = self.pinecone_client.Index(name)\n\n    return index\n</code></pre>"},{"location":"api/#whyhow.rag.Client.get_index","title":"<code>get_index(name)</code>","text":"<p>Get an existing index.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the index.</p> required <p>Returns:</p> Type Description <code>Index</code> <p>The index.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the index does not exist.</p> Source code in <code>whyhow/rag.py</code> <pre><code>def get_index(self, name: str) -&gt; Index:\n    \"\"\"Get an existing index.\n\n    Parameters\n    ----------\n    name : str\n        The name of the index.\n\n\n    Returns\n    -------\n    Index\n        The index.\n\n    Raises\n    ------\n    ValueError\n        If the index does not exist.\n\n    \"\"\"\n    try:\n        index = self.pinecone_client.Index(name)\n    except NotFoundException as e:\n        raise IndexNotFoundException(f\"Index {name} does not exist\") from e\n\n    return index\n</code></pre>"},{"location":"api/#whyhow.rag.Client.query","title":"<code>query(question, index, namespace, rules=None, top_k=5, chat_model='gpt-3.5-turbo', chat_temperature=0.0, chat_max_tokens=1000, chat_seed=2, embedding_model='text-embedding-ada-002')</code>","text":"<p>Query the index.</p> Source code in <code>whyhow/rag.py</code> <pre><code>    def query(\n        self,\n        question: str,\n        index: Index,\n        namespace: str,\n        rules: list[Rule] | None = None,\n        top_k: int = 5,\n        chat_model: str = \"gpt-3.5-turbo\",\n        chat_temperature: float = 0.0,\n        chat_max_tokens: int = 1000,\n        chat_seed: int = 2,\n        embedding_model: str = \"text-embedding-ada-002\",\n    ) -&gt; QueryReturnType:\n        \"\"\"Query the index.\"\"\"\n        if rules is None:\n            rules = []\n\n        filters = [rule.to_filter() for rule in rules]\n        filters = [f for f in filters if f is not None]\n        logger.info(f\"Using {len(filters)} filters (from {len(rules)} rules)\")\n        if not filters:\n            filter = None\n        else:\n            filter = {\"$or\": [f for f in filters]}\n\n        question_embedding = generate_embeddings(\n            openai_api_key=self.openai_client.api_key,\n            chunks=[question],\n            model=embedding_model,\n        )[0]\n\n        query_response = index.query(\n            namespace=namespace,\n            top_k=top_k,\n            vector=question_embedding,\n            filter=filter,\n            include_metadata=True,\n        )\n        matches = [\n            PineconeMatch(**m.to_dict()) for m in query_response[\"matches\"]\n        ]\n\n        texts = [m.metadata.text for m in matches]\n        input_actual = Input(question=question, contexts=texts)\n\n        prompt_end = f\"\"\"\nACTUAL INPUT\n```json\n{input_actual.model_dump_json()}\n```\n\nACTUAL OUTPUT\n\"\"\"\n        prompt = f\"{PROMPT_START}\\n{prompt_end}\"\n        logger.debug(f\"Prompt: {prompt}\")\n\n        response = self.openai_client.chat.completions.create(\n            model=chat_model,\n            seed=chat_seed,\n            temperature=chat_temperature,\n            messages=[{\"role\": \"user\", \"content\": prompt}],\n            max_tokens=chat_max_tokens,\n        )\n\n        choice = response.choices[0]\n        if choice.finish_reason != \"stop\":\n            raise OpenAIException(\n                f\"Chat did not finish. Reason: {choice.finish_reason}\"\n            )\n\n        response_raw = cast(str, response.choices[0].message.content)\n\n        if response_raw.startswith(\"```json\"):\n            start_i = response_raw.index(\"{\")\n            end_i = response_raw.rindex(\"}\")\n            response_raw = response_raw[start_i : end_i + 1]\n\n        try:\n            output = Output.model_validate_json(response_raw)\n        except ValidationError as e:\n            raise OpenAIException(\n                f\"OpenAI did not return a valid JSON: {response_raw}\"\n            ) from e\n\n        return_dict: QueryReturnType = {\n            \"answer\": output.answer,\n            \"matches\": [m.model_dump() for m in matches],\n            \"used_contexts\": output.contexts,\n        }\n\n        return return_dict\n</code></pre>"},{"location":"api/#whyhow.rag.Client.upload_documents","title":"<code>upload_documents(index, documents, namespace, embedding_model='text-embedding-ada-002', batch_size=100)</code>","text":"<p>Upload documents to the index.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>Index</code> <p>The index.</p> required <code>documents</code> <code>list[str | Path]</code> <p>The documents to upload.</p> required <code>namespace</code> <code>str</code> <p>The namespace within the index to use.</p> required <code>batch_size</code> <code>int</code> <p>The number of documents to upload at a time.</p> <code>100</code> <code>embedding_model</code> <code>str</code> <p>The OpenAI embedding model to use.</p> <code>'text-embedding-ada-002'</code> Source code in <code>whyhow/rag.py</code> <pre><code>def upload_documents(\n    self,\n    index: Index,\n    documents: list[str | pathlib.Path],\n    namespace: str,\n    embedding_model: str = \"text-embedding-ada-002\",\n    batch_size: int = 100,\n) -&gt; None:\n    \"\"\"Upload documents to the index.\n\n    Parameters\n    ----------\n    index : Index\n        The index.\n\n    documents : list[str | pathlib.Path]\n        The documents to upload.\n\n    namespace : str\n        The namespace within the index to use.\n\n    batch_size : int\n        The number of documents to upload at a time.\n\n    embedding_model : str\n        The OpenAI embedding model to use.\n\n    \"\"\"\n    # don't allow for duplicate documents\n    documents = list(set(documents))\n    if not documents:\n        logger.info(\"No documents to upload\")\n        return\n\n    logger.info(f\"Parsing {len(documents)} documents\")\n    all_chunks: list[Document] = []\n    for document in documents:\n        chunks_ = parse_and_split(document)\n        chunks = clean_chunks(chunks_)\n        all_chunks.extend(chunks)\n\n    logger.info(f\"Embedding {len(all_chunks)} chunks\")\n    embeddings = generate_embeddings(\n        openai_api_key=self.openai_client.api_key,\n        chunks=[c.page_content for c in all_chunks],\n        model=embedding_model,\n    )\n\n    if len(embeddings) != len(all_chunks):\n        raise ValueError(\n            \"Number of embeddings does not match number of chunks\"\n        )\n\n    # create PineconeDocuments\n    pinecone_documents = []\n    for i, (chunk, embedding) in enumerate(zip(all_chunks, embeddings)):\n        metadata = PineconeMetadata(\n            text=chunk.page_content,\n            page_number=chunk.metadata[\"page\"],\n            chunk_number=chunk.metadata[\"chunk\"],\n            filename=chunk.metadata[\"source\"],\n        )\n        pinecone_document = PineconeDocument(\n            values=embedding,\n            metadata=metadata,\n        )\n        pinecone_documents.append(pinecone_document)\n\n    upsert_documents = [d.model_dump() for d in pinecone_documents]\n\n    response = index.upsert(\n        upsert_documents, namespace=namespace, batch_size=batch_size\n    )\n    n_upserted = response[\"upserted_count\"]\n    logger.info(f\"Upserted {n_upserted} documents\")\n</code></pre>"},{"location":"api/#whyhow.rag.Rule","title":"<code>whyhow.rag.Rule</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Retrieval rule.</p> Source code in <code>whyhow/rag.py</code> <pre><code>class Rule(BaseModel):\n    \"\"\"Retrieval rule.\"\"\"\n\n    filename: str | None = None\n    uuid: str | None = None\n    page_numbers: list[int] | None = None\n\n    @field_validator(\"page_numbers\", mode=\"before\")\n    @classmethod\n    def convert_empty_to_none(cls, v: list[int] | None) -&gt; list[int] | None:\n        \"\"\"Convert empty list to None.\"\"\"\n        if v is not None and not v:\n            return None\n        return v\n\n    def to_filter(self) -&gt; dict[str, list[dict[str, Any]]] | None:\n        \"\"\"Convert rule to Pinecone filter format.\"\"\"\n        if not any([self.filename, self.uuid, self.page_numbers]):\n            return None\n\n        conditions: list[dict[str, Any]] = []\n        if self.filename is not None:\n            conditions.append({\"filename\": {\"$eq\": self.filename}})\n        if self.uuid is not None:\n            conditions.append({\"uuid\": {\"$eq\": self.uuid}})\n        if self.page_numbers is not None:\n            conditions.append({\"page_number\": {\"$in\": self.page_numbers}})\n\n        filter_ = {\"$and\": conditions}\n        return filter_\n</code></pre>"},{"location":"api/#whyhow.rag.Rule.convert_empty_to_none","title":"<code>convert_empty_to_none(v)</code>  <code>classmethod</code>","text":"<p>Convert empty list to None.</p> Source code in <code>whyhow/rag.py</code> <pre><code>@field_validator(\"page_numbers\", mode=\"before\")\n@classmethod\ndef convert_empty_to_none(cls, v: list[int] | None) -&gt; list[int] | None:\n    \"\"\"Convert empty list to None.\"\"\"\n    if v is not None and not v:\n        return None\n    return v\n</code></pre>"},{"location":"api/#whyhow.rag.Rule.to_filter","title":"<code>to_filter()</code>","text":"<p>Convert rule to Pinecone filter format.</p> Source code in <code>whyhow/rag.py</code> <pre><code>def to_filter(self) -&gt; dict[str, list[dict[str, Any]]] | None:\n    \"\"\"Convert rule to Pinecone filter format.\"\"\"\n    if not any([self.filename, self.uuid, self.page_numbers]):\n        return None\n\n    conditions: list[dict[str, Any]] = []\n    if self.filename is not None:\n        conditions.append({\"filename\": {\"$eq\": self.filename}})\n    if self.uuid is not None:\n        conditions.append({\"uuid\": {\"$eq\": self.uuid}})\n    if self.page_numbers is not None:\n        conditions.append({\"page_number\": {\"$in\": self.page_numbers}})\n\n    filter_ = {\"$and\": conditions}\n    return filter_\n</code></pre>"},{"location":"api/#whyhow.rag.PineconeMetadata","title":"<code>whyhow.rag.PineconeMetadata</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>The metadata to be stored in Pinecone.</p> Source code in <code>whyhow/rag.py</code> <pre><code>class PineconeMetadata(BaseModel, extra=\"forbid\"):\n    \"\"\"The metadata to be stored in Pinecone.\"\"\"\n\n    text: str\n    page_number: int\n    chunk_number: int\n    filename: str\n    uuid: str = Field(default_factory=lambda: str(uuid.uuid4()))\n</code></pre>"},{"location":"api/#whyhow.rag.PineconeDocument","title":"<code>whyhow.rag.PineconeDocument</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>The actual document to be stored in Pinecone.</p> Source code in <code>whyhow/rag.py</code> <pre><code>class PineconeDocument(BaseModel, extra=\"forbid\"):\n    \"\"\"The actual document to be stored in Pinecone.\"\"\"\n\n    metadata: PineconeMetadata\n    values: list[float] | None = None\n    id: str | None = None\n\n    @model_validator(mode=\"after\")\n    def generate_human_readable_id(self) -&gt; \"PineconeDocument\":\n        \"\"\"Generate a human-readable identifier for the document.\"\"\"\n        if self.id is None:\n            meta = self.metadata\n            hr_id = f\"{meta.filename}-{meta.page_number}-{meta.chunk_number}\"\n            self.id = hr_id\n\n        return self\n</code></pre>"},{"location":"api/#whyhow.rag.PineconeDocument.generate_human_readable_id","title":"<code>generate_human_readable_id()</code>","text":"<p>Generate a human-readable identifier for the document.</p> Source code in <code>whyhow/rag.py</code> <pre><code>@model_validator(mode=\"after\")\ndef generate_human_readable_id(self) -&gt; \"PineconeDocument\":\n    \"\"\"Generate a human-readable identifier for the document.\"\"\"\n    if self.id is None:\n        meta = self.metadata\n        hr_id = f\"{meta.filename}-{meta.page_number}-{meta.chunk_number}\"\n        self.id = hr_id\n\n    return self\n</code></pre>"},{"location":"api/#whyhow.rag.PineconeMatch","title":"<code>whyhow.rag.PineconeMatch</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>The match returned from Pinecone.</p> Source code in <code>whyhow/rag.py</code> <pre><code>class PineconeMatch(BaseModel, extra=\"ignore\"):\n    \"\"\"The match returned from Pinecone.\"\"\"\n\n    id: str\n    score: float\n    metadata: PineconeMetadata\n</code></pre>"},{"location":"api/#whyhow.rag.Input","title":"<code>whyhow.rag.Input</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>The input for the prompt.</p> Source code in <code>whyhow/rag.py</code> <pre><code>class Input(BaseModel):\n    \"\"\"The input for the prompt.\"\"\"\n\n    question: str\n    contexts: list[str]\n</code></pre>"},{"location":"api/#whyhow.rag.Output","title":"<code>whyhow.rag.Output</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>The output of the prompt.</p> Source code in <code>whyhow/rag.py</code> <pre><code>class Output(BaseModel):\n    \"\"\"The output of the prompt.\"\"\"\n\n    answer: str\n    contexts: list[int]\n</code></pre>"},{"location":"api/#whyhow.rag.Client.query","title":"<code>whyhow.rag.Client.query(question, index, namespace, rules=None, top_k=5, chat_model='gpt-3.5-turbo', chat_temperature=0.0, chat_max_tokens=1000, chat_seed=2, embedding_model='text-embedding-ada-002')</code>","text":"<p>Query the index.</p> Source code in <code>whyhow/rag.py</code> <pre><code>    def query(\n        self,\n        question: str,\n        index: Index,\n        namespace: str,\n        rules: list[Rule] | None = None,\n        top_k: int = 5,\n        chat_model: str = \"gpt-3.5-turbo\",\n        chat_temperature: float = 0.0,\n        chat_max_tokens: int = 1000,\n        chat_seed: int = 2,\n        embedding_model: str = \"text-embedding-ada-002\",\n    ) -&gt; QueryReturnType:\n        \"\"\"Query the index.\"\"\"\n        if rules is None:\n            rules = []\n\n        filters = [rule.to_filter() for rule in rules]\n        filters = [f for f in filters if f is not None]\n        logger.info(f\"Using {len(filters)} filters (from {len(rules)} rules)\")\n        if not filters:\n            filter = None\n        else:\n            filter = {\"$or\": [f for f in filters]}\n\n        question_embedding = generate_embeddings(\n            openai_api_key=self.openai_client.api_key,\n            chunks=[question],\n            model=embedding_model,\n        )[0]\n\n        query_response = index.query(\n            namespace=namespace,\n            top_k=top_k,\n            vector=question_embedding,\n            filter=filter,\n            include_metadata=True,\n        )\n        matches = [\n            PineconeMatch(**m.to_dict()) for m in query_response[\"matches\"]\n        ]\n\n        texts = [m.metadata.text for m in matches]\n        input_actual = Input(question=question, contexts=texts)\n\n        prompt_end = f\"\"\"\nACTUAL INPUT\n```json\n{input_actual.model_dump_json()}\n```\n\nACTUAL OUTPUT\n\"\"\"\n        prompt = f\"{PROMPT_START}\\n{prompt_end}\"\n        logger.debug(f\"Prompt: {prompt}\")\n\n        response = self.openai_client.chat.completions.create(\n            model=chat_model,\n            seed=chat_seed,\n            temperature=chat_temperature,\n            messages=[{\"role\": \"user\", \"content\": prompt}],\n            max_tokens=chat_max_tokens,\n        )\n\n        choice = response.choices[0]\n        if choice.finish_reason != \"stop\":\n            raise OpenAIException(\n                f\"Chat did not finish. Reason: {choice.finish_reason}\"\n            )\n\n        response_raw = cast(str, response.choices[0].message.content)\n\n        if response_raw.startswith(\"```json\"):\n            start_i = response_raw.index(\"{\")\n            end_i = response_raw.rindex(\"}\")\n            response_raw = response_raw[start_i : end_i + 1]\n\n        try:\n            output = Output.model_validate_json(response_raw)\n        except ValidationError as e:\n            raise OpenAIException(\n                f\"OpenAI did not return a valid JSON: {response_raw}\"\n            ) from e\n\n        return_dict: QueryReturnType = {\n            \"answer\": output.answer,\n            \"matches\": [m.model_dump() for m in matches],\n            \"used_contexts\": output.contexts,\n        }\n\n        return return_dict\n</code></pre>"},{"location":"installation/","title":"Installation","text":"<p>To install the Rule-based Retrieval package, follow these steps:</p>"},{"location":"installation/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.10 or higher</li> <li>OpenAI API key</li> <li>Pinecone API key</li> </ul>"},{"location":"installation/#install-from-github","title":"Install from GitHub","text":"<p>Clone the repository:</p> <pre><code>git clone git@github.com:whyhow-ai/rule-based-retrieval.git\ncd rule-based-retrieval\n</code></pre> <p>Install the packages:</p> <pre><code>pip install .\n</code></pre> <p>Set the required environment variables:</p> <pre><code>export OPENAI_API_KEY=&lt;your open ai api key&gt;\nexport PINECONE_API_KEY=&lt;your pinecone api key&gt;\n</code></pre>"},{"location":"installation/#developer-installation","title":"Developer Installation","text":"<p>For a developer installation, use an editable install and include the development dependencies:</p> <pre><code>pip install -e .[dev]\n</code></pre> <p>For ZSH:</p> <pre><code>pip install -e \".[dev]\"\n</code></pre>"},{"location":"installation/#install-documentation-dependencies","title":"Install Documentation Dependencies","text":"<p>To build and serve the documentation locally, install the documentation dependencies:</p> <pre><code>pip install -e .[docs]\n</code></pre> <p>For ZSH:</p> <pre><code>pip install -e \".[docs]\"\n</code></pre> <p>Then, use mkdocs to serve the documentation:</p> <pre><code>mkdocs serve\n</code></pre> <p>Navigate to http://127.0.0.1:8000/ in your browser to view the documentation.</p>"},{"location":"installation/#troubleshooting","title":"Troubleshooting","text":"<p>If you encounter any issues during installation, please check the following:</p> <ul> <li>Ensure that you have Python 3.10 or higher installed. You can check your Python version by running <code>python --version</code> in your terminal.</li> <li>Make sure that you have correctly set the <code>OPENAI_API_KEY</code> and <code>PINECONE_API_KEY</code> environment variables with your respective API keys.</li> <li>If you are installing from GitHub, ensure that you have cloned the repository correctly and are in the right directory.</li> <li>If you are using a virtual environment, make sure that it is activated before running the installation commands.</li> <li>If you still face problems, please open an issue on the GitHub repository with detailed information about the error and your environment setup.</li> </ul>"},{"location":"tutorial/","title":"Tutorial","text":"<p>The <code>whyhow</code> package helps create customized RAG pipelines. It is built on top of the following technologies (and their respective Python SDKs)</p> <ul> <li>OpenAI - text generation</li> <li>Pinecone - vector database</li> </ul>"},{"location":"tutorial/#client","title":"Client","text":"<p>The central object is a <code>Client</code>. It manages all necessary resources and provides a simple interface for all the RAG related tasks.</p> <p>First of all, to instantiate it one needs to provide the following API keys:</p> <ul> <li><code>OPENAI_API_KEY</code></li> <li><code>PINECONE_API_KEY</code></li> </ul> <p>One can either define the corresponding environment variables</p> <pre><code>export OPENAI_API_KEY=...\nexport PINECONE_API_KEY...\n</code></pre> <p>and then instantiate the client without any arguments.</p> getting_started.py<pre><code>from whyhow import Client\n\nclient = Client()\n</code></pre> <pre><code>python getting_started.py\n</code></pre> <p>An alternative approach is to manually pass the keys when the client is being constructed</p> getting_started.py<pre><code>from whyhow import Client\n\nclient = Client(\n    openai_api_key=\"...\",\n    pinecone_api_key=\"...\"\n\n)\n</code></pre> <pre><code>python getting_started.py\n</code></pre>"},{"location":"tutorial/#vector-database-operations","title":"Vector database operations","text":"<p><code>whywow</code> uses Pinecone for everything related to vector databses.</p>"},{"location":"tutorial/#creating-an-index","title":"Creating an index","text":"<p>If you don't have a Pinecone index yet, you can create it using the <code>create_index</code> method of the <code>Client</code>. This method is a thin wrapper around the Pinecone SDK (official docs).</p> <p>First of all, you need to provide a specification. There are 2 types</p> <ul> <li>Serverless</li> <li>Pod-based</li> </ul>"},{"location":"tutorial/#serverless","title":"Serverless","text":"<p>To create a serverless index you can use</p> <pre><code># Code above omitted \ud83d\udc46\n\nfrom pinecone import ServerlessSpec\n\nspec = ServerlessSpec(\n    cloud=\"aws\",\n    region=\"us-west-2\"\n)\n\nindex = client.create_index(\n    name=\"great-index\",  # the only required argument\n    dimension=1536\n    metric=\"cosine\",\n    spec=spec\n)\n</code></pre> Full code <pre><code>from pinecone import ServerlessSpec\n\nfrom whyhow import Client\n\nclient = Client()\n\nspec = ServerlessSpec(\n    cloud=\"aws\",\n    region=\"us-west-2\"\n)\n\nindex = client.create_index(\n    name=\"great-index\",  # the only required argument\n    dimension=1536\n    metric=\"cosine\",\n    spec=spec\n)\n</code></pre>"},{"location":"tutorial/#pod-based","title":"Pod-based","text":"<p>To create a pod-based index you can use</p> <pre><code># Code above omitted \ud83d\udc46\n\nfrom pinecone import PodSpec\n\nspec = PodSpec(\n    environment=\"gcp-starter\"\n)\n\nindex = client.create_index(\n    name=\"amazing-index\",  # the only required argument\n    dimension=1536\n    metric=\"cosine\",\n    spec=spec\n)\n</code></pre> Full code <pre><code>from pinecone import PodSpec\n\nfrom whyhow import Client\n\nclient = Client()\n\nspec = PodSpec(\n    environment=\"gcp-starter\"\n)\n\nindex = client.create_index(\n    name=\"amazing-index\",  # the only required argument\n    dimension=1536\n    metric=\"cosine\",\n    spec=spec\n)\n</code></pre> <p>Info</p> <p>For detailed information on what all of the parameters mean please refer to (Pinecone) Understanding indices</p>"},{"location":"tutorial/#getting-an-existing-index","title":"Getting an existing index","text":"<p>If your exists already, you can use the <code>get_index</code> method to get it.</p> <pre><code># Code above omitted \ud83d\udc46\n\nindex = client.get_index(\"amazing-index\")\n</code></pre> Full code <pre><code>from pinecone import PodSpec\n\nfrom whyhow import Client\n\nclient = Client()\n\nindex = client.get_index(\"amazing-index\")\n</code></pre>"},{"location":"tutorial/#index-operations","title":"Index operations","text":"<p>Both <code>create_index</code> and <code>get_index</code> return an instance of <code>pinecone.Index</code>. It offers multiple convenience methods. See below a few examples.</p>"},{"location":"tutorial/#describe_index_stats","title":"<code>describe_index_stats</code>","text":"<p>Shows useful information about the index.</p> <pre><code>index.describe_index_stats()\n</code></pre> <p>Example output:</p> <pre><code>{'dimension': 1536,\n 'index_fullness': 0.00448,\n 'namespaces': {'A': {'vector_count': 11},\n                'B': {'vector_count': 11},\n                'C': {'vector_count': 62},\n                'D': {'vector_count': 82},\n                'E': {'vector_count': 282}},\n 'total_vector_count': 448}\n</code></pre>"},{"location":"tutorial/#fetch","title":"<code>fetch</code>","text":"<p>Fetch (Pinecone docs)</p>"},{"location":"tutorial/#upsert","title":"<code>upsert</code>","text":"<p>Upsert (Pinecone docs)</p>"},{"location":"tutorial/#query","title":"<code>query</code>","text":"<p>Query (Pinecone docs)</p>"},{"location":"tutorial/#delete","title":"<code>delete</code>","text":"<p>Delete (Pinecone docs)</p>"},{"location":"tutorial/#update","title":"<code>update</code>","text":"<p>Update (Pinecone docs)</p>"},{"location":"tutorial/#uploading-documents","title":"Uploading documents","text":"<p>After creating an index, we are ready to populate it with documents. In <code>whyhow</code> this is done using the <code>upload_documents</code> method of the <code>Client</code>. It performs the following steps under the hood:</p> <ul> <li>Preprocessing: Reading and splitting the provided PDF files into chunks</li> <li>Embedding: Embedding all the chunks using an OpenAI model</li> <li>Upserting: Uploading both the embeddings and the metadata to a Pinecone index</li> </ul> <p>See below an example of how to use it.</p> <pre><code># Code above omitted \ud83d\udc46\n\nnamespace = \"books\"\npdfs = [\"harry-potter.pdf\", \"game-of-thrones.pdf\"]\n\nclient.upload_documents(\n    index=index,\n    documents=pdfs,\n    namespace=namespace\n)\n</code></pre> Full code <pre><code>from whyhow import Client\n\nclient = Client()\n\nindex = client.get_index(\"amazing-index\")\n\nnamespace = \"books\"\npdfs = [\"harry-potter.pdf\", \"game-of-thrones.pdf\"]\n\nclient.upload_documents(\n    index=index,\n    documents=pdfs,\n    namespace=namespace\n)\n</code></pre> <p>Warning</p> <p>The above example assumes you have two PDFs on your disk.</p> <ul> <li><code>harry-potter.pdf</code></li> <li><code>game-of-thrones.pdf</code></li> </ul> <p>However, feel free to provide different documents.</p> <p>Info</p> <p>The <code>upload_documents</code> method does not return anything. If you want to get some information about what is going on you can activate logging.</p> <p><pre><code>import logging\n\nlogging.basicConfig(\n    level=logging.WARNING,\n    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n)\n</code></pre> Note that the above affects the root logger, however, you can also just customize the <code>whyhow</code> logger.</p> <p>Navigate to upload_documents (API docs) if you want to get more information on the parameters.</p>"},{"location":"tutorial/#index-schema","title":"Index schema","text":"<p>While Pinecone does not require each document in an index to have the same schema all the document uploaded via the <code>upload_documents</code> will have a fixed schema. This schema is defined in PineconeDocument (API docs). This is done in order to have a predictable set of attributes that can be used to perform advanced filtering (via rules).</p>"},{"location":"tutorial/#question-answering","title":"Question answering","text":"<p>In previous sections we discussed how to to create an index and populate it with documents. Now we can finally move to retrieval augmented generation.</p> <p>In <code>whyhow</code>, it can be done via the <code>query</code> method.</p> <ol> <li>Simple example:</li> </ol> <pre><code>from whyhow import Client, Rule\n\nclient = Client()\n\nindex = client.get_index(\"amazing-index\")\nnamespace = \"books\"\n\nquestion = \"What is Harry Potter's favorite food?\"\n\nrule = Rule(\n    filename=\"harry-potter.pdf\",\n    page_numbers=[120, 121, 150]\n)\n\nresult = client.query(\n    question=question,\n    index=index,\n    namespace=namespace,\n    rules=[rule]\n)\n\nprint(result[\"answer\"])\nprint(result[\"matches\"])\nprint(result[\"used_contexts\"])\n</code></pre> <p>The <code>result</code> is a dictionary that has the following three keys</p> <ul> <li><code>answer</code> - the the answer to the question</li> <li><code>matches</code> - the <code>top_k</code> most relevant documents from the index</li> <li><code>used_contexts</code> - the matches (or more precisely just the texts/contexts) that   the LLM used to answer the question.</li> </ul> <pre><code>print(result[\"answer\"])\n</code></pre> <pre><code>'Treacle tart'\n</code></pre> <pre><code>print(result[\"matches\"])\n</code></pre> <pre><code>[{'id': 'harry-potter.pdf-120-5',\n  'metadata': {'chunk_number': 5,\n               'filename': 'harry-potter.pdf',\n               'page_number': 120,\n               'text': 'Harry loves the treacle tart.'\n               'uuid': '86314e32-7d88-475c-b950-f8c156ebf259'},\n  'score': 0.826438308},\n {'id': 'game-of-thrones.pdf-75-1',\n  'metadata': {'chunk_number': 1,\n               'filename': 'game-of-thrones.pdf',\n               'page_number': 75,\n               'text': 'Harry Strickland was the head of the exiled House Strickland.'\n                       'He enjoys eating roasted beef.'\n               'uuid': '684a978b-e6e7-45e2-8ba4-5c5019c7c676'},\n  'score': 0.2052352},\n  ...\n  ]\n</code></pre> <p>Note that the number of matches will be in general equal to <code>top_k</code> which can be specified as a parameter. Also, each match has a fixed schema - it is a dump of PineconeMatch (API docs).</p> <pre><code>print(result[\"used_contexts\"])\n</code></pre> <pre><code>[0]\n</code></pre> <p>The OpenAI model only used the context from the 1st match when answering the question.</p> Full code <pre><code>from whyhow import Client\n\nclient = Client()\n\nindex = client.get_index(\"amazing-index\")\n\nnamespace = \"books\"\n\nquestion = \"What is Harry Potter's favourite food?\"\n\nresult = client.query(\n    question=question\n    index=index,\n    namespace=namespace\n)\n\nprint(result[\"answer\"])\nprint(result[\"matches\"])\nprint(result[\"used_contexts\"])\n</code></pre> <p>Navigate to query(API docs) if you want to get more information on the parameters.</p>"},{"location":"tutorial/#rules","title":"Rules","text":"<p>In the previous example, every single document in our index was considered. However, sometimes it might be beneficial to only retrieve documents satisfying some predefined conditions (e.g. <code>filename=harry-potter.pdf</code>). In <code>whyhow</code> this can be done via the <code>Rule</code> class.</p> <p>A rule can control the following metadata attributes</p> <ul> <li><code>filename</code> - name of the file</li> <li><code>page_numbers</code> - list of integers corresponding to page numbers (0 indexing)</li> <li><code>uuid</code> - unique identifier of a chunk (this is the most \"extreme\" filter)</li> <li> <p><code>keywords</code> - list of keywords to trigger the rule</p> </li> <li> <p>Keyword example:</p> </li> </ul> <pre><code># Code above omitted \ud83d\udc46\n\nfrom whyhow import Rule\n\nquestion = \"What is Harry Potter's favourite food?\"\n\nrule = Rule(\n    filename=\"harry-potter.pdf\",\n    page_numbers=[120, 121, 150],\n    keywords=[\"food\", \"favorite\", \"likes to eat\"]\n)\nresult = client.query(\n    question=question\n    index=index,\n    namespace=namespace,\n    rules=[rule],\n    keyword_trigger=True\n)\n</code></pre> <p>In this example, the keyword_trigger parameter is set to True, and the rule includes keywords. Only the rules whose keywords match the words in the question will be applied.</p> Full code <pre><code>from whyhow import Client, Rule\n\nclient = Client()\n\nindex = client.get_index(\"amazing-index\")\nnamespace = \"books\"\n\nquestion = \"What does Harry Potter like to eat?\"\n\nrule = Rule(\n    filename=\"harry-potter.pdf\",\n    keywords=[\"food\", \"favorite\", \"likes to eat\"]\n)\n\nresult = client.query(\n    question=question,\n    index=index,\n    namespace=namespace,\n    rules=[rule],\n    keyword_trigger=True\n)\n\nprint(result[\"answer\"])\nprint(result[\"matches\"])\nprint(result[\"used_contexts\"])\n</code></pre> <ol> <li>Process rules separately example:</li> </ol> <p>Lastly, you can specify multiple rules at the same time. They will be evaluated using the <code>OR</code> logical operator.</p> <pre><code># Code above omitted \ud83d\udc46\n\nfrom whyhow import Rule\n\nquestion = \"What is Harry Potter's favorite food?\"\n\nrule_1 = Rule(\n    filename=\"harry-potter.pdf\",\n    page_numbers=[120, 121, 150]\n)\n\nrule_2 = Rule(\n    filename=\"harry-potter-volume-2.pdf\",\n    page_numbers=[80, 81, 82]\n)\n\nresult = client.query(\n    question=question,\n    index=index,\n    namespace=namespace,\n    rules=[rule_1, rule_2],\n    process_rules_separately=True\n)\n</code></pre> <p>In this example, the process_rules_separately parameter is set to True. This means that each rule (rule_1 and rule_2) will be processed independently, ensuring that both rules contribute to the final result set.</p> <p>By default, all rules are run as one joined query, which means that one rule can dominate the others, and given the limit by top_k, a lower priority rule might not return any results. However, by setting process_rules_separately to True, each rule will be processed independently, ensuring that every rule returns results, and the results will be combined at the end.</p> <p>Depending on the number of rules you use in your query, you may return more chunks than your LLM\u2019s context window can handle. Be mindful of your model\u2019s token limits and adjust your top_k and rule count accordingly.</p> Full code <pre><code>from whyhow import Client, Rule\n\nclient = Client()\n\nindex = client.get_index(\"amazing-index\")\nnamespace = \"books\"\n\nquestion = \"What is Harry Potter's favorite food?\"\n\nrule_1 = Rule(\n    filename=\"harry-potter.pdf\",\n    page_numbers=[120, 121, 150]\n)\n\nrule_2 = Rule(\n    filename=\"harry-potter-volume-2.pdf\",\n    page_numbers=[80, 81, 82]\n)\n\nresult = client.query(\n    question=question,\n    index=index,\n    namespace=namespace,\n    rules=[rule_1, rule_2],\n    process_rules_separately=True\n)\n\nprint(result[\"answer\"])\nprint(result[\"matches\"])\nprint(result[\"used_contexts\"])\n</code></pre> <p>Navigate to Rule (API docs) if you want to get more information on the parameters.</p>"}]}