{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Rule-based Retrieval Documentation","text":"<p>The Rule-based Retrieval package is a Python package for creating Retrieval Augmented Generation (RAG) applications with filtering capabilities. It leverages OpenAI for text generation and Pinecone for vector database management.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Easy-to-use API for creating and managing Pinecone indexes</li> <li>Uploading and processing documents (currently supports PDF files)</li> <li>Generating embeddings using OpenAI models</li> <li>Querying the index with custom filtering rules</li> <li>Retrieval Augmented Generation for question answering</li> <li>Querying the index with custom filtering rules, including processing rules separately and triggering rules based on keywords</li> </ul>"},{"location":"#getting-started","title":"Getting Started","text":"<ol> <li>Install the package by following the Installation Guide</li> <li>Set up your OpenAI and Pinecone API keys as environment variables</li> <li>Create an index and upload your documents using the <code>Client</code> class</li> <li>Query the index with custom rules to retrieve relevant documents, optionally processing rules separately or triggering rules based on keywords</li> <li>Use the retrieved documents to generate answers to your questions</li> </ol> <p>For a detailed walkthrough and code examples, check out the Tutorial.</p>"},{"location":"#architecture-overview","title":"Architecture Overview","text":"<p>The Rule-based Retrieval package consists of the following main components:</p> <ul> <li><code>Client</code>: The central class for managing resources and performing RAG-related tasks</li> <li><code>Rule</code>: Allows defining custom filtering rules for retrieving documents</li> <li><code>PineconeMetadata</code> and <code>PineconeDocument</code>: Classes for representing and storing document metadata and embeddings in Pinecone</li> <li><code>embedding</code>, <code>processing</code>, and <code>exceptions</code> modules: Utility functions and custom exceptions</li> </ul>"},{"location":"api/","title":"Reference","text":""},{"location":"api/#whyhowembedding-module","title":"<code>whyhow.embedding</code> module","text":""},{"location":"api/#whyhow_rbr.embedding.generate_embeddings","title":"<code>whyhow_rbr.embedding.generate_embeddings(openai_api_key, chunks, model='text-embedding-3-small')</code>","text":"<p>Generate embeddings for a list of chunks.</p> <p>Parameters:</p> Name Type Description Default <code>openai_api_key</code> <code>str</code> <p>OpenAI API key.</p> required <code>chunks</code> <code>list[str]</code> <p>List of chunks to generate embeddings for.</p> required <code>model</code> <code>str</code> <p>OpenAI model to use for generating embeddings.</p> <code>'text-embedding-3-small'</code> <p>Returns:</p> Type Description <code>list[list[float]]</code> <p>List of embeddings for each chunk.</p> Source code in <code>whyhow_rbr/embedding.py</code> <pre><code>def generate_embeddings(\n    openai_api_key: str,\n    chunks: list[str],\n    model: str = \"text-embedding-3-small\",\n) -&gt; list[list[float]]:\n    \"\"\"Generate embeddings for a list of chunks.\n\n    Parameters\n    ----------\n    openai_api_key : str\n        OpenAI API key.\n\n    chunks : list[str]\n        List of chunks to generate embeddings for.\n\n    model : str\n        OpenAI model to use for generating embeddings.\n\n    Returns\n    -------\n    list[list[float]]\n        List of embeddings for each chunk.\n\n    \"\"\"\n    embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key, model=model)  # type: ignore[call-arg]\n    embeddings_array = embeddings.embed_documents(chunks)\n\n    return embeddings_array\n</code></pre>"},{"location":"api/#whyhowexceptions-module","title":"<code>whyhow.exceptions</code> module","text":""},{"location":"api/#whyhow_rbr.exceptions.IndexAlreadyExistsException","title":"<code>whyhow_rbr.exceptions.IndexAlreadyExistsException</code>","text":"<p>             Bases: <code>Exception</code></p> <p>Raised when the index already exists.</p> Source code in <code>whyhow_rbr/exceptions.py</code> <pre><code>class IndexAlreadyExistsException(Exception):\n    \"\"\"Raised when the index already exists.\"\"\"\n\n    pass\n</code></pre>"},{"location":"api/#whyhow_rbr.exceptions.IndexNotFoundException","title":"<code>whyhow_rbr.exceptions.IndexNotFoundException</code>","text":"<p>             Bases: <code>Exception</code></p> <p>Raised when the index is not found.</p> Source code in <code>whyhow_rbr/exceptions.py</code> <pre><code>class IndexNotFoundException(Exception):\n    \"\"\"Raised when the index is not found.\"\"\"\n\n    pass\n</code></pre>"},{"location":"api/#whyhow_rbr.exceptions.OpenAIException","title":"<code>whyhow_rbr.exceptions.OpenAIException</code>","text":"<p>             Bases: <code>Exception</code></p> <p>Raised when the OpenAI API returns an error.</p> Source code in <code>whyhow_rbr/exceptions.py</code> <pre><code>class OpenAIException(Exception):\n    \"\"\"Raised when the OpenAI API returns an error.\"\"\"\n\n    pass\n</code></pre>"},{"location":"api/#whyhowprocessing-module","title":"<code>whyhow.processing</code> module","text":""},{"location":"api/#whyhow_rbr.processing.parse_and_split","title":"<code>whyhow_rbr.processing.parse_and_split(path, chunk_size=512, chunk_overlap=100)</code>","text":"<p>Parse a PDF and split it into chunks.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str or Path</code> <p>Path to the document to process.</p> required <code>chunk_size</code> <code>int</code> <p>Size of the chunks.</p> <code>512</code> <code>chunk_overlap</code> <code>int</code> <p>Overlap between chunks.</p> <code>100</code> <p>Returns:</p> Type Description <code>list[Document]</code> <p>The chunks of the pdf.</p> Source code in <code>whyhow_rbr/processing.py</code> <pre><code>def parse_and_split(\n    path: str | pathlib.Path,\n    chunk_size: int = 512,\n    chunk_overlap: int = 100,\n) -&gt; list[Document]:\n    \"\"\"Parse a PDF and split it into chunks.\n\n    Parameters\n    ----------\n    path : str or pathlib.Path\n        Path to the document to process.\n\n    chunk_size : int\n        Size of the chunks.\n\n    chunk_overlap : int\n        Overlap between chunks.\n\n    Returns\n    -------\n    list[Document]\n        The chunks of the pdf.\n    \"\"\"\n    loader = PyPDFLoader(str(path))\n    docs = loader.load()\n    splitter = RecursiveCharacterTextSplitter(\n        chunk_size=chunk_size,\n        chunk_overlap=chunk_overlap,\n    )\n    chunks = splitter.split_documents(docs)\n\n    # Assign the change number (within a page) to each chunk\n    i_page = 0\n    i_chunk = 0\n\n    for chunk in chunks:\n        if chunk.metadata[\"page\"] != i_page:\n            i_page = chunk.metadata[\"page\"]\n            i_chunk = 0\n\n        chunk.metadata[\"chunk\"] = i_chunk\n        i_chunk += 1\n\n    return chunks\n</code></pre>"},{"location":"api/#whyhow_rbr.processing.clean_chunks","title":"<code>whyhow_rbr.processing.clean_chunks(chunks)</code>","text":"<p>Clean the chunks of a pdf.</p> <p>No modifications in-place.</p> <p>Parameters:</p> Name Type Description Default <code>chunks</code> <code>list[Document]</code> <p>The chunks of the pdf.</p> required <p>Returns:</p> Type Description <code>list[Document]</code> <p>The cleaned chunks.</p> Source code in <code>whyhow_rbr/processing.py</code> <pre><code>def clean_chunks(\n    chunks: list[Document],\n) -&gt; list[Document]:\n    \"\"\"Clean the chunks of a pdf.\n\n    No modifications in-place.\n\n    Parameters\n    ----------\n    chunks : list[Document]\n        The chunks of the pdf.\n\n    Returns\n    -------\n    list[Document]\n        The cleaned chunks.\n    \"\"\"\n    pattern = re.compile(r\"(\\r\\n|\\n|\\r)\")\n    clean_chunks: list[Document] = []\n\n    for chunk in chunks:\n        text = re.sub(pattern, \"\", chunk.page_content)\n        new_chunk = Document(\n            page_content=text,\n            metadata=copy.deepcopy(chunk.metadata),\n        )\n\n        clean_chunks.append(new_chunk)\n\n    return clean_chunks\n</code></pre>"},{"location":"api/#whyhowrag-module","title":"<code>whyhow.rag</code> module","text":"<pre><code>:docstring:\n:members:\n:undoc-members:\n:show-inheritance:\n:special-members: __init__\n</code></pre>"},{"location":"api/#whyhow_rbr.rag.Client","title":"<code>whyhow_rbr.rag.Client</code>","text":"<p>Synchronous client.</p> Source code in <code>whyhow_rbr/rag.py</code> <pre><code>class Client:\n    \"\"\"Synchronous client.\"\"\"\n\n    def __init__(\n        self,\n        openai_api_key: str | None = None,\n        pinecone_api_key: str | None = None,\n    ):\n        if openai_api_key is None:\n            openai_api_key = os.environ.get(\"OPENAI_API_KEY\")\n            if openai_api_key is None:\n                raise ValueError(\n                    \"No OPENAI_API_KEY provided must be provided.\"\n                )\n\n        if pinecone_api_key is None:\n            pinecone_api_key = os.environ.get(\"PINECONE_API_KEY\")\n            if pinecone_api_key is None:\n                raise ValueError(\"No PINECONE_API_KEY provided\")\n\n        self.openai_client = OpenAI(api_key=openai_api_key)\n        self.pinecone_client = Pinecone(api_key=pinecone_api_key)\n\n    def get_index(self, name: str) -&gt; Index:\n        \"\"\"Get an existing index.\n\n        Parameters\n        ----------\n        name : str\n            The name of the index.\n\n\n        Returns\n        -------\n        Index\n            The index.\n\n        Raises\n        ------\n        IndexNotFoundException\n            If the index does not exist.\n\n        \"\"\"\n        try:\n            index = self.pinecone_client.Index(name)\n        except NotFoundException as e:\n            raise IndexNotFoundException(f\"Index {name} does not exist\") from e\n\n        return index\n\n    def create_index(\n        self,\n        name: str,\n        dimension: int = 1536,\n        metric: Metric = \"cosine\",\n        spec: ServerlessSpec | PodSpec | None = None,\n    ) -&gt; Index:\n        \"\"\"Create a new index.\n\n        If the index does not exist, it creates a new index with the specified.\n\n        Parameters\n        ----------\n        name : str\n            The name of the index.\n\n        dimension : int\n            The dimension of the index.\n\n        metric : Metric\n            The metric of the index.\n\n        spec : ServerlessSpec | PodSpec | None\n            The spec of the index. If None, it uses the default spec.\n\n        Raises\n        ------\n        IndexAlreadyExistsException\n            If the index already exists.\n\n        \"\"\"\n        try:\n            self.get_index(name)\n        except IndexNotFoundException:\n            pass\n        else:\n            raise IndexAlreadyExistsException(f\"Index {name} already exists\")\n\n        if spec is None:\n            spec = DEFAULT_SPEC\n            logger.info(f\"Using default spec {spec}\")\n\n        self.pinecone_client.create_index(\n            name=name, dimension=dimension, metric=metric, spec=spec\n        )\n        index = self.pinecone_client.Index(name)\n\n        return index\n\n    def upload_documents(\n        self,\n        index: Index,\n        documents: list[str | pathlib.Path],\n        namespace: str,\n        embedding_model: str = \"text-embedding-3-small\",\n        batch_size: int = 100,\n    ) -&gt; None:\n        \"\"\"Upload documents to the index.\n\n        Parameters\n        ----------\n        index : Index\n            The index.\n\n        documents : list[str | pathlib.Path]\n            The documents to upload.\n\n        namespace : str\n            The namespace within the index to use.\n\n        batch_size : int\n            The number of documents to upload at a time.\n\n        embedding_model : str\n            The OpenAI embedding model to use.\n\n        \"\"\"\n        # don't allow for duplicate documents\n        documents = list(set(documents))\n        if not documents:\n            logger.info(\"No documents to upload\")\n            return\n\n        logger.info(f\"Parsing {len(documents)} documents\")\n        all_chunks: list[Document] = []\n        for document in documents:\n            chunks_ = parse_and_split(document)\n            chunks = clean_chunks(chunks_)\n            all_chunks.extend(chunks)\n\n        logger.info(f\"Embedding {len(all_chunks)} chunks\")\n        embeddings = generate_embeddings(\n            openai_api_key=self.openai_client.api_key,\n            chunks=[c.page_content for c in all_chunks],\n            model=embedding_model,\n        )\n\n        if len(embeddings) != len(all_chunks):\n            raise ValueError(\n                \"Number of embeddings does not match number of chunks\"\n            )\n\n        # create PineconeDocuments\n        pinecone_documents = []\n        for i, (chunk, embedding) in enumerate(zip(all_chunks, embeddings)):\n            metadata = PineconeMetadata(\n                text=chunk.page_content,\n                page_number=chunk.metadata[\"page\"],\n                chunk_number=chunk.metadata[\"chunk\"],\n                filename=chunk.metadata[\"source\"],\n            )\n            pinecone_document = PineconeDocument(\n                values=embedding,\n                metadata=metadata,\n            )\n            pinecone_documents.append(pinecone_document)\n\n        upsert_documents = [d.model_dump() for d in pinecone_documents]\n\n        response = index.upsert(\n            upsert_documents, namespace=namespace, batch_size=batch_size\n        )\n        n_upserted = response[\"upserted_count\"]\n        logger.info(f\"Upserted {n_upserted} documents\")\n\n    def clean_text(self, text: str) -&gt; str:\n        \"\"\"Return a lower case version of text with punctuation removed.\n\n        Parameters\n        ----------\n        text : str\n            The raw text to be cleaned.\n\n        Returns\n        -------\n        str: The cleaned text string.\n        \"\"\"\n        text_processed = re.sub(\"[^0-9a-zA-Z ]+\", \"\", text.lower())\n        text_processed_further = re.sub(\" +\", \" \", text_processed)\n        return text_processed_further\n\n    def query(\n        self,\n        question: str,\n        index: Index,\n        namespace: str,\n        rules: list[Rule] | None = None,\n        top_k: int = 5,\n        chat_model: str = \"gpt-4-1106-preview\",\n        chat_temperature: float = 0.0,\n        chat_max_tokens: int = 1000,\n        chat_seed: int = 2,\n        embedding_model: str = \"text-embedding-3-small\",\n        process_rules_separately: bool = False,\n        keyword_trigger: bool = False,\n    ) -&gt; QueryReturnType:\n        \"\"\"Query the index.\n\n        Parameters\n        ----------\n        question : str\n            The question to ask.\n\n        index : Index\n            The index to query.\n\n        namespace : str\n            The namespace within the index to use.\n\n        rules : list[Rule] | None\n            The rules to use for filtering the documents.\n\n        top_k : int\n            The number of matches to return per rule.\n\n        chat_model : str\n            The OpenAI chat model to use.\n\n        chat_temperature : float\n            The temperature for the chat model.\n\n        chat_max_tokens : int\n            The maximum number of tokens for the chat model.\n\n        chat_seed : int\n            The seed for the chat model.\n\n        embedding_model : str\n            The OpenAI embedding model to use.\n\n        process_rules_separately : bool, optional\n            Whether to process each rule individually and combine the results at the end.\n            When set to True, each rule will be run independently, ensuring that every rule\n            returns results. When set to False (default), all rules will be run as one joined\n            query, potentially allowing one rule to dominate the others.\n            Default is False.\n\n        keyword_trigger : bool, optional\n            Whether to trigger rules based on keyword matches in the question.\n            Default is False.\n\n        Returns\n        -------\n        QueryReturnType\n            Dictionary with keys \"answer\", \"matches\", and \"used_contexts\".\n            The \"answer\" is the answer to the question.\n            The \"matches\" are the \"top_k\" matches from the index.\n            The \"used_contexts\" are the indices of the matches\n            that were actually used to answer the question.\n\n        Raises\n        ------\n        OpenAIException\n            If there is an error with the OpenAI API. Some possible reasons\n            include the chat model not finishing or the response not being\n            valid JSON.\n        \"\"\"\n        logger.info(f\"Raw rules: {rules}\")\n\n        if rules is None:\n            rules = []\n\n        if keyword_trigger:\n            triggered_rules = []\n            clean_question = self.clean_text(question).split(\" \")\n\n            for rule in rules:\n                if rule.keywords:\n                    clean_keywords = [\n                        self.clean_text(keyword) for keyword in rule.keywords\n                    ]\n\n                    if bool(set(clean_keywords) &amp; set(clean_question)):\n                        triggered_rules.append(rule)\n\n            rules = triggered_rules\n\n        rule_filters = [rule.to_filter() for rule in rules if rule is not None]\n\n        question_embedding = generate_embeddings(\n            openai_api_key=self.openai_client.api_key,\n            chunks=[question],\n            model=embedding_model,\n        )[0]\n\n        matches = (\n            []\n        )  # Initialize matches outside the loop to collect matches from all queries\n        match_texts = []\n\n        # Check if there are any rule filters, and if not, proceed with a default query\n        if not rule_filters:\n            # Perform a default query\n            query_response = index.query(\n                namespace=namespace,\n                top_k=top_k,\n                vector=question_embedding,\n                filter=None,  # No specific filter, or you can define a default filter as per your application's logic\n                include_metadata=True,\n            )\n            matches = [\n                PineconeMatch(**m.to_dict()) for m in query_response[\"matches\"]\n            ]\n            match_texts = [m.metadata.text for m in matches]\n\n        else:\n\n            if process_rules_separately:\n                for rule_filter in rule_filters:\n                    if rule_filter:\n                        query_response = index.query(\n                            namespace=namespace,\n                            top_k=top_k,\n                            vector=question_embedding,\n                            filter=rule_filter,\n                            include_metadata=True,\n                        )\n                        matches.extend(\n                            [\n                                PineconeMatch(**m.to_dict())\n                                for m in query_response[\"matches\"]\n                            ]\n                        )\n                        match_texts += [m.metadata.text for m in matches]\n                match_texts = list(\n                    set(match_texts)\n                )  # Ensure unique match texts\n            else:\n                if rule_filters:\n                    combined_filters = []\n                    for rule_filter in rule_filters:\n                        if rule_filter:\n                            combined_filters.append(rule_filter)\n\n                    rule_filter = (\n                        {\"$or\": combined_filters} if combined_filters else None\n                    )\n                else:\n                    rule_filter = None  # Fallback to a default query when no rules are provided or valid\n\n                if rule_filter is not None:\n                    query_response = index.query(\n                        namespace=namespace,\n                        top_k=top_k,\n                        vector=question_embedding,\n                        filter=rule_filter,\n                        include_metadata=True,\n                    )\n                    matches = [\n                        PineconeMatch(**m.to_dict())\n                        for m in query_response[\"matches\"]\n                    ]\n                    match_texts = [m.metadata.text for m in matches]\n\n        # Proceed to create prompt, send it to OpenAI, and handle the response\n        prompt = self.create_prompt(question, match_texts)\n        response = self.openai_client.chat.completions.create(\n            model=chat_model,\n            seed=chat_seed,\n            temperature=chat_temperature,\n            messages=[{\"role\": \"user\", \"content\": prompt}],\n            max_tokens=chat_max_tokens,\n        )\n\n        output = self.process_response(response)\n\n        return_dict: QueryReturnType = {\n            \"answer\": output.answer,\n            \"matches\": [m.model_dump() for m in matches],\n            \"used_contexts\": output.contexts,\n        }\n\n        return return_dict\n\n    def create_prompt(self, question: str, match_texts: list[str]) -&gt; str:\n        \"\"\"Create the prompt for the OpenAI chat completion.\n\n        Parameters\n        ----------\n        question : str\n            The question to ask.\n\n        match_texts : list[str]\n            The list of context strings to include in the prompt.\n\n        Returns\n        -------\n        str\n            The generated prompt.\n        \"\"\"\n        input_actual = Input(question=question, contexts=match_texts)\n        prompt_end = f\"\"\"\n        ACTUAL INPUT\n        ```json\n        {input_actual.model_dump_json()}\n        ```\n\n        ACTUAL OUTPUT\n        \"\"\"\n        return f\"{PROMPT_START}\\n{prompt_end}\"\n\n    def process_response(self, response: Any) -&gt; Output:\n        \"\"\"Process the OpenAI chat completion response.\n\n        Parameters\n        ----------\n        response : Any\n            The OpenAI chat completion response.\n\n        Returns\n        -------\n        Output\n            The processed output.\n\n        Raises\n        ------\n        OpenAIException\n            If the chat model did not finish or the response is not valid JSON.\n        \"\"\"\n        choice = response.choices[0]\n        if choice.finish_reason != \"stop\":\n            raise OpenAIException(\n                f\"Chat did not finish. Reason: {choice.finish_reason}\"\n            )\n\n        response_raw = cast(str, response.choices[0].message.content)\n\n        if response_raw.startswith(\"```json\"):\n            start_i = response_raw.index(\"{\")\n            end_i = response_raw.rindex(\"}\")\n            response_raw = response_raw[start_i : end_i + 1]\n\n        try:\n            output = Output.model_validate_json(response_raw)\n        except ValidationError as e:\n            raise OpenAIException(\n                f\"OpenAI did not return a valid JSON: {response_raw}\"\n            ) from e\n\n        return output\n</code></pre>"},{"location":"api/#whyhow_rbr.rag.Client.clean_text","title":"<code>clean_text(text)</code>","text":"<p>Return a lower case version of text with punctuation removed.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The raw text to be cleaned.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>The cleaned text string.</code> Source code in <code>whyhow_rbr/rag.py</code> <pre><code>def clean_text(self, text: str) -&gt; str:\n    \"\"\"Return a lower case version of text with punctuation removed.\n\n    Parameters\n    ----------\n    text : str\n        The raw text to be cleaned.\n\n    Returns\n    -------\n    str: The cleaned text string.\n    \"\"\"\n    text_processed = re.sub(\"[^0-9a-zA-Z ]+\", \"\", text.lower())\n    text_processed_further = re.sub(\" +\", \" \", text_processed)\n    return text_processed_further\n</code></pre>"},{"location":"api/#whyhow_rbr.rag.Client.create_index","title":"<code>create_index(name, dimension=1536, metric='cosine', spec=None)</code>","text":"<p>Create a new index.</p> <p>If the index does not exist, it creates a new index with the specified.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the index.</p> required <code>dimension</code> <code>int</code> <p>The dimension of the index.</p> <code>1536</code> <code>metric</code> <code>Metric</code> <p>The metric of the index.</p> <code>'cosine'</code> <code>spec</code> <code>ServerlessSpec | PodSpec | None</code> <p>The spec of the index. If None, it uses the default spec.</p> <code>None</code> <p>Raises:</p> Type Description <code>IndexAlreadyExistsException</code> <p>If the index already exists.</p> Source code in <code>whyhow_rbr/rag.py</code> <pre><code>def create_index(\n    self,\n    name: str,\n    dimension: int = 1536,\n    metric: Metric = \"cosine\",\n    spec: ServerlessSpec | PodSpec | None = None,\n) -&gt; Index:\n    \"\"\"Create a new index.\n\n    If the index does not exist, it creates a new index with the specified.\n\n    Parameters\n    ----------\n    name : str\n        The name of the index.\n\n    dimension : int\n        The dimension of the index.\n\n    metric : Metric\n        The metric of the index.\n\n    spec : ServerlessSpec | PodSpec | None\n        The spec of the index. If None, it uses the default spec.\n\n    Raises\n    ------\n    IndexAlreadyExistsException\n        If the index already exists.\n\n    \"\"\"\n    try:\n        self.get_index(name)\n    except IndexNotFoundException:\n        pass\n    else:\n        raise IndexAlreadyExistsException(f\"Index {name} already exists\")\n\n    if spec is None:\n        spec = DEFAULT_SPEC\n        logger.info(f\"Using default spec {spec}\")\n\n    self.pinecone_client.create_index(\n        name=name, dimension=dimension, metric=metric, spec=spec\n    )\n    index = self.pinecone_client.Index(name)\n\n    return index\n</code></pre>"},{"location":"api/#whyhow_rbr.rag.Client.create_prompt","title":"<code>create_prompt(question, match_texts)</code>","text":"<p>Create the prompt for the OpenAI chat completion.</p> <p>Parameters:</p> Name Type Description Default <code>question</code> <code>str</code> <p>The question to ask.</p> required <code>match_texts</code> <code>list[str]</code> <p>The list of context strings to include in the prompt.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The generated prompt.</p> Source code in <code>whyhow_rbr/rag.py</code> <pre><code>def create_prompt(self, question: str, match_texts: list[str]) -&gt; str:\n    \"\"\"Create the prompt for the OpenAI chat completion.\n\n    Parameters\n    ----------\n    question : str\n        The question to ask.\n\n    match_texts : list[str]\n        The list of context strings to include in the prompt.\n\n    Returns\n    -------\n    str\n        The generated prompt.\n    \"\"\"\n    input_actual = Input(question=question, contexts=match_texts)\n    prompt_end = f\"\"\"\n    ACTUAL INPUT\n    ```json\n    {input_actual.model_dump_json()}\n    ```\n\n    ACTUAL OUTPUT\n    \"\"\"\n    return f\"{PROMPT_START}\\n{prompt_end}\"\n</code></pre>"},{"location":"api/#whyhow_rbr.rag.Client.get_index","title":"<code>get_index(name)</code>","text":"<p>Get an existing index.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the index.</p> required <p>Returns:</p> Type Description <code>Index</code> <p>The index.</p> <p>Raises:</p> Type Description <code>IndexNotFoundException</code> <p>If the index does not exist.</p> Source code in <code>whyhow_rbr/rag.py</code> <pre><code>def get_index(self, name: str) -&gt; Index:\n    \"\"\"Get an existing index.\n\n    Parameters\n    ----------\n    name : str\n        The name of the index.\n\n\n    Returns\n    -------\n    Index\n        The index.\n\n    Raises\n    ------\n    IndexNotFoundException\n        If the index does not exist.\n\n    \"\"\"\n    try:\n        index = self.pinecone_client.Index(name)\n    except NotFoundException as e:\n        raise IndexNotFoundException(f\"Index {name} does not exist\") from e\n\n    return index\n</code></pre>"},{"location":"api/#whyhow_rbr.rag.Client.process_response","title":"<code>process_response(response)</code>","text":"<p>Process the OpenAI chat completion response.</p> <p>Parameters:</p> Name Type Description Default <code>response</code> <code>Any</code> <p>The OpenAI chat completion response.</p> required <p>Returns:</p> Type Description <code>Output</code> <p>The processed output.</p> <p>Raises:</p> Type Description <code>OpenAIException</code> <p>If the chat model did not finish or the response is not valid JSON.</p> Source code in <code>whyhow_rbr/rag.py</code> <pre><code>def process_response(self, response: Any) -&gt; Output:\n    \"\"\"Process the OpenAI chat completion response.\n\n    Parameters\n    ----------\n    response : Any\n        The OpenAI chat completion response.\n\n    Returns\n    -------\n    Output\n        The processed output.\n\n    Raises\n    ------\n    OpenAIException\n        If the chat model did not finish or the response is not valid JSON.\n    \"\"\"\n    choice = response.choices[0]\n    if choice.finish_reason != \"stop\":\n        raise OpenAIException(\n            f\"Chat did not finish. Reason: {choice.finish_reason}\"\n        )\n\n    response_raw = cast(str, response.choices[0].message.content)\n\n    if response_raw.startswith(\"```json\"):\n        start_i = response_raw.index(\"{\")\n        end_i = response_raw.rindex(\"}\")\n        response_raw = response_raw[start_i : end_i + 1]\n\n    try:\n        output = Output.model_validate_json(response_raw)\n    except ValidationError as e:\n        raise OpenAIException(\n            f\"OpenAI did not return a valid JSON: {response_raw}\"\n        ) from e\n\n    return output\n</code></pre>"},{"location":"api/#whyhow_rbr.rag.Client.query","title":"<code>query(question, index, namespace, rules=None, top_k=5, chat_model='gpt-4-1106-preview', chat_temperature=0.0, chat_max_tokens=1000, chat_seed=2, embedding_model='text-embedding-3-small', process_rules_separately=False, keyword_trigger=False)</code>","text":"<p>Query the index.</p> <p>Parameters:</p> Name Type Description Default <code>question</code> <code>str</code> <p>The question to ask.</p> required <code>index</code> <code>Index</code> <p>The index to query.</p> required <code>namespace</code> <code>str</code> <p>The namespace within the index to use.</p> required <code>rules</code> <code>list[Rule] | None</code> <p>The rules to use for filtering the documents.</p> <code>None</code> <code>top_k</code> <code>int</code> <p>The number of matches to return per rule.</p> <code>5</code> <code>chat_model</code> <code>str</code> <p>The OpenAI chat model to use.</p> <code>'gpt-4-1106-preview'</code> <code>chat_temperature</code> <code>float</code> <p>The temperature for the chat model.</p> <code>0.0</code> <code>chat_max_tokens</code> <code>int</code> <p>The maximum number of tokens for the chat model.</p> <code>1000</code> <code>chat_seed</code> <code>int</code> <p>The seed for the chat model.</p> <code>2</code> <code>embedding_model</code> <code>str</code> <p>The OpenAI embedding model to use.</p> <code>'text-embedding-3-small'</code> <code>process_rules_separately</code> <code>bool</code> <p>Whether to process each rule individually and combine the results at the end. When set to True, each rule will be run independently, ensuring that every rule returns results. When set to False (default), all rules will be run as one joined query, potentially allowing one rule to dominate the others. Default is False.</p> <code>False</code> <code>keyword_trigger</code> <code>bool</code> <p>Whether to trigger rules based on keyword matches in the question. Default is False.</p> <code>False</code> <p>Returns:</p> Type Description <code>QueryReturnType</code> <p>Dictionary with keys \"answer\", \"matches\", and \"used_contexts\". The \"answer\" is the answer to the question. The \"matches\" are the \"top_k\" matches from the index. The \"used_contexts\" are the indices of the matches that were actually used to answer the question.</p> <p>Raises:</p> Type Description <code>OpenAIException</code> <p>If there is an error with the OpenAI API. Some possible reasons include the chat model not finishing or the response not being valid JSON.</p> Source code in <code>whyhow_rbr/rag.py</code> <pre><code>def query(\n    self,\n    question: str,\n    index: Index,\n    namespace: str,\n    rules: list[Rule] | None = None,\n    top_k: int = 5,\n    chat_model: str = \"gpt-4-1106-preview\",\n    chat_temperature: float = 0.0,\n    chat_max_tokens: int = 1000,\n    chat_seed: int = 2,\n    embedding_model: str = \"text-embedding-3-small\",\n    process_rules_separately: bool = False,\n    keyword_trigger: bool = False,\n) -&gt; QueryReturnType:\n    \"\"\"Query the index.\n\n    Parameters\n    ----------\n    question : str\n        The question to ask.\n\n    index : Index\n        The index to query.\n\n    namespace : str\n        The namespace within the index to use.\n\n    rules : list[Rule] | None\n        The rules to use for filtering the documents.\n\n    top_k : int\n        The number of matches to return per rule.\n\n    chat_model : str\n        The OpenAI chat model to use.\n\n    chat_temperature : float\n        The temperature for the chat model.\n\n    chat_max_tokens : int\n        The maximum number of tokens for the chat model.\n\n    chat_seed : int\n        The seed for the chat model.\n\n    embedding_model : str\n        The OpenAI embedding model to use.\n\n    process_rules_separately : bool, optional\n        Whether to process each rule individually and combine the results at the end.\n        When set to True, each rule will be run independently, ensuring that every rule\n        returns results. When set to False (default), all rules will be run as one joined\n        query, potentially allowing one rule to dominate the others.\n        Default is False.\n\n    keyword_trigger : bool, optional\n        Whether to trigger rules based on keyword matches in the question.\n        Default is False.\n\n    Returns\n    -------\n    QueryReturnType\n        Dictionary with keys \"answer\", \"matches\", and \"used_contexts\".\n        The \"answer\" is the answer to the question.\n        The \"matches\" are the \"top_k\" matches from the index.\n        The \"used_contexts\" are the indices of the matches\n        that were actually used to answer the question.\n\n    Raises\n    ------\n    OpenAIException\n        If there is an error with the OpenAI API. Some possible reasons\n        include the chat model not finishing or the response not being\n        valid JSON.\n    \"\"\"\n    logger.info(f\"Raw rules: {rules}\")\n\n    if rules is None:\n        rules = []\n\n    if keyword_trigger:\n        triggered_rules = []\n        clean_question = self.clean_text(question).split(\" \")\n\n        for rule in rules:\n            if rule.keywords:\n                clean_keywords = [\n                    self.clean_text(keyword) for keyword in rule.keywords\n                ]\n\n                if bool(set(clean_keywords) &amp; set(clean_question)):\n                    triggered_rules.append(rule)\n\n        rules = triggered_rules\n\n    rule_filters = [rule.to_filter() for rule in rules if rule is not None]\n\n    question_embedding = generate_embeddings(\n        openai_api_key=self.openai_client.api_key,\n        chunks=[question],\n        model=embedding_model,\n    )[0]\n\n    matches = (\n        []\n    )  # Initialize matches outside the loop to collect matches from all queries\n    match_texts = []\n\n    # Check if there are any rule filters, and if not, proceed with a default query\n    if not rule_filters:\n        # Perform a default query\n        query_response = index.query(\n            namespace=namespace,\n            top_k=top_k,\n            vector=question_embedding,\n            filter=None,  # No specific filter, or you can define a default filter as per your application's logic\n            include_metadata=True,\n        )\n        matches = [\n            PineconeMatch(**m.to_dict()) for m in query_response[\"matches\"]\n        ]\n        match_texts = [m.metadata.text for m in matches]\n\n    else:\n\n        if process_rules_separately:\n            for rule_filter in rule_filters:\n                if rule_filter:\n                    query_response = index.query(\n                        namespace=namespace,\n                        top_k=top_k,\n                        vector=question_embedding,\n                        filter=rule_filter,\n                        include_metadata=True,\n                    )\n                    matches.extend(\n                        [\n                            PineconeMatch(**m.to_dict())\n                            for m in query_response[\"matches\"]\n                        ]\n                    )\n                    match_texts += [m.metadata.text for m in matches]\n            match_texts = list(\n                set(match_texts)\n            )  # Ensure unique match texts\n        else:\n            if rule_filters:\n                combined_filters = []\n                for rule_filter in rule_filters:\n                    if rule_filter:\n                        combined_filters.append(rule_filter)\n\n                rule_filter = (\n                    {\"$or\": combined_filters} if combined_filters else None\n                )\n            else:\n                rule_filter = None  # Fallback to a default query when no rules are provided or valid\n\n            if rule_filter is not None:\n                query_response = index.query(\n                    namespace=namespace,\n                    top_k=top_k,\n                    vector=question_embedding,\n                    filter=rule_filter,\n                    include_metadata=True,\n                )\n                matches = [\n                    PineconeMatch(**m.to_dict())\n                    for m in query_response[\"matches\"]\n                ]\n                match_texts = [m.metadata.text for m in matches]\n\n    # Proceed to create prompt, send it to OpenAI, and handle the response\n    prompt = self.create_prompt(question, match_texts)\n    response = self.openai_client.chat.completions.create(\n        model=chat_model,\n        seed=chat_seed,\n        temperature=chat_temperature,\n        messages=[{\"role\": \"user\", \"content\": prompt}],\n        max_tokens=chat_max_tokens,\n    )\n\n    output = self.process_response(response)\n\n    return_dict: QueryReturnType = {\n        \"answer\": output.answer,\n        \"matches\": [m.model_dump() for m in matches],\n        \"used_contexts\": output.contexts,\n    }\n\n    return return_dict\n</code></pre>"},{"location":"api/#whyhow_rbr.rag.Client.upload_documents","title":"<code>upload_documents(index, documents, namespace, embedding_model='text-embedding-3-small', batch_size=100)</code>","text":"<p>Upload documents to the index.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>Index</code> <p>The index.</p> required <code>documents</code> <code>list[str | Path]</code> <p>The documents to upload.</p> required <code>namespace</code> <code>str</code> <p>The namespace within the index to use.</p> required <code>batch_size</code> <code>int</code> <p>The number of documents to upload at a time.</p> <code>100</code> <code>embedding_model</code> <code>str</code> <p>The OpenAI embedding model to use.</p> <code>'text-embedding-3-small'</code> Source code in <code>whyhow_rbr/rag.py</code> <pre><code>def upload_documents(\n    self,\n    index: Index,\n    documents: list[str | pathlib.Path],\n    namespace: str,\n    embedding_model: str = \"text-embedding-3-small\",\n    batch_size: int = 100,\n) -&gt; None:\n    \"\"\"Upload documents to the index.\n\n    Parameters\n    ----------\n    index : Index\n        The index.\n\n    documents : list[str | pathlib.Path]\n        The documents to upload.\n\n    namespace : str\n        The namespace within the index to use.\n\n    batch_size : int\n        The number of documents to upload at a time.\n\n    embedding_model : str\n        The OpenAI embedding model to use.\n\n    \"\"\"\n    # don't allow for duplicate documents\n    documents = list(set(documents))\n    if not documents:\n        logger.info(\"No documents to upload\")\n        return\n\n    logger.info(f\"Parsing {len(documents)} documents\")\n    all_chunks: list[Document] = []\n    for document in documents:\n        chunks_ = parse_and_split(document)\n        chunks = clean_chunks(chunks_)\n        all_chunks.extend(chunks)\n\n    logger.info(f\"Embedding {len(all_chunks)} chunks\")\n    embeddings = generate_embeddings(\n        openai_api_key=self.openai_client.api_key,\n        chunks=[c.page_content for c in all_chunks],\n        model=embedding_model,\n    )\n\n    if len(embeddings) != len(all_chunks):\n        raise ValueError(\n            \"Number of embeddings does not match number of chunks\"\n        )\n\n    # create PineconeDocuments\n    pinecone_documents = []\n    for i, (chunk, embedding) in enumerate(zip(all_chunks, embeddings)):\n        metadata = PineconeMetadata(\n            text=chunk.page_content,\n            page_number=chunk.metadata[\"page\"],\n            chunk_number=chunk.metadata[\"chunk\"],\n            filename=chunk.metadata[\"source\"],\n        )\n        pinecone_document = PineconeDocument(\n            values=embedding,\n            metadata=metadata,\n        )\n        pinecone_documents.append(pinecone_document)\n\n    upsert_documents = [d.model_dump() for d in pinecone_documents]\n\n    response = index.upsert(\n        upsert_documents, namespace=namespace, batch_size=batch_size\n    )\n    n_upserted = response[\"upserted_count\"]\n    logger.info(f\"Upserted {n_upserted} documents\")\n</code></pre>"},{"location":"api/#whyhow_rbr.rag.Rule","title":"<code>whyhow_rbr.rag.Rule</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Retrieval rule.</p> <p>The rule is used to filter the documents in the index.</p> <p>Attributes:</p> Name Type Description <code>filename</code> <code>str | None</code> <p>The filename of the document.</p> <code>uuid</code> <code>str | None</code> <p>The UUID of the document.</p> <code>page_numbers</code> <code>list[int] | None</code> <p>The page numbers of the document.</p> <code>keywords</code> <code>list[str] | None</code> <p>The keywords to trigger a rule.</p> Source code in <code>whyhow_rbr/rag.py</code> <pre><code>class Rule(BaseModel):\n    \"\"\"Retrieval rule.\n\n    The rule is used to filter the documents in the index.\n\n    Attributes\n    ----------\n    filename : str | None\n        The filename of the document.\n\n    uuid : str | None\n        The UUID of the document.\n\n    page_numbers : list[int] | None\n        The page numbers of the document.\n\n    keywords : list[str] | None\n        The keywords to trigger a rule.\n    \"\"\"\n\n    filename: str | None = None\n    uuid: str | None = None\n    page_numbers: list[int] | None = None\n    keywords: list[str] | None = None\n\n    @field_validator(\"page_numbers\", mode=\"before\")\n    @classmethod\n    def convert_empty_to_none(cls, v: list[int] | None) -&gt; list[int] | None:\n        \"\"\"Convert empty list to None.\"\"\"\n        if v is not None and not v:\n            return None\n        return v\n\n    def convert_empty_str_to_none(\n        cls, s: list[str] | None\n    ) -&gt; list[str] | None:\n        \"\"\"Convert empty string list to None.\"\"\"\n        if s is not None and not s:\n            return None\n        return s\n\n    def to_filter(self) -&gt; dict[str, list[dict[str, Any]]] | None:\n        \"\"\"Convert rule to Pinecone filter format.\"\"\"\n        if not any([self.filename, self.uuid, self.page_numbers]):\n            return None\n\n        conditions: list[dict[str, Any]] = []\n        if self.filename is not None:\n            conditions.append({\"filename\": {\"$eq\": self.filename}})\n        if self.uuid is not None:\n            conditions.append({\"uuid\": {\"$eq\": self.uuid}})\n        if self.page_numbers is not None:\n            conditions.append({\"page_number\": {\"$in\": self.page_numbers}})\n\n        filter_ = {\"$and\": conditions}\n        return filter_\n</code></pre>"},{"location":"api/#whyhow_rbr.rag.Rule.convert_empty_str_to_none","title":"<code>convert_empty_str_to_none(s)</code>","text":"<p>Convert empty string list to None.</p> Source code in <code>whyhow_rbr/rag.py</code> <pre><code>def convert_empty_str_to_none(\n    cls, s: list[str] | None\n) -&gt; list[str] | None:\n    \"\"\"Convert empty string list to None.\"\"\"\n    if s is not None and not s:\n        return None\n    return s\n</code></pre>"},{"location":"api/#whyhow_rbr.rag.Rule.convert_empty_to_none","title":"<code>convert_empty_to_none(v)</code>  <code>classmethod</code>","text":"<p>Convert empty list to None.</p> Source code in <code>whyhow_rbr/rag.py</code> <pre><code>@field_validator(\"page_numbers\", mode=\"before\")\n@classmethod\ndef convert_empty_to_none(cls, v: list[int] | None) -&gt; list[int] | None:\n    \"\"\"Convert empty list to None.\"\"\"\n    if v is not None and not v:\n        return None\n    return v\n</code></pre>"},{"location":"api/#whyhow_rbr.rag.Rule.to_filter","title":"<code>to_filter()</code>","text":"<p>Convert rule to Pinecone filter format.</p> Source code in <code>whyhow_rbr/rag.py</code> <pre><code>def to_filter(self) -&gt; dict[str, list[dict[str, Any]]] | None:\n    \"\"\"Convert rule to Pinecone filter format.\"\"\"\n    if not any([self.filename, self.uuid, self.page_numbers]):\n        return None\n\n    conditions: list[dict[str, Any]] = []\n    if self.filename is not None:\n        conditions.append({\"filename\": {\"$eq\": self.filename}})\n    if self.uuid is not None:\n        conditions.append({\"uuid\": {\"$eq\": self.uuid}})\n    if self.page_numbers is not None:\n        conditions.append({\"page_number\": {\"$in\": self.page_numbers}})\n\n    filter_ = {\"$and\": conditions}\n    return filter_\n</code></pre>"},{"location":"api/#whyhow_rbr.rag.PineconeMetadata","title":"<code>whyhow_rbr.rag.PineconeMetadata</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>The metadata to be stored in Pinecone.</p> <p>Attributes:</p> Name Type Description <code>text</code> <code>str</code> <p>The text of the document.</p> <code>page_number</code> <code>int</code> <p>The page number of the document.</p> <code>chunk_number</code> <code>int</code> <p>The chunk number of the document.</p> <code>filename</code> <code>str</code> <p>The filename of the document.</p> <code>uuid</code> <code>str</code> <p>The UUID of the document. Note that this is not required to be provided when creating the metadata. It is generated automatically when creating the PineconeDocument.</p> Source code in <code>whyhow_rbr/rag.py</code> <pre><code>class PineconeMetadata(BaseModel, extra=\"forbid\"):\n    \"\"\"The metadata to be stored in Pinecone.\n\n    Attributes\n    ----------\n    text : str\n        The text of the document.\n\n    page_number : int\n        The page number of the document.\n\n    chunk_number : int\n        The chunk number of the document.\n\n    filename : str\n        The filename of the document.\n\n    uuid : str\n        The UUID of the document. Note that this is not required to be\n        provided when creating the metadata. It is generated automatically\n        when creating the PineconeDocument.\n    \"\"\"\n\n    text: str\n    page_number: int\n    chunk_number: int\n    filename: str\n    uuid: str = Field(default_factory=lambda: str(uuid.uuid4()))\n</code></pre>"},{"location":"api/#whyhow_rbr.rag.PineconeDocument","title":"<code>whyhow_rbr.rag.PineconeDocument</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>The actual document to be stored in Pinecone.</p> <p>Attributes:</p> Name Type Description <code>metadata</code> <code>PineconeMetadata</code> <p>The metadata of the document.</p> <code>values</code> <code>list[float] | None</code> <p>The embedding of the document. The None is used when querying the index since the values are not needed. At upsert time, the values are required.</p> <code>id</code> <code>str | None</code> <p>The human-readable identifier of the document. This is generated automatically when creating the PineconeDocument unless it is provided.</p> Source code in <code>whyhow_rbr/rag.py</code> <pre><code>class PineconeDocument(BaseModel, extra=\"forbid\"):\n    \"\"\"The actual document to be stored in Pinecone.\n\n    Attributes\n    ----------\n    metadata : PineconeMetadata\n        The metadata of the document.\n\n    values : list[float] | None\n        The embedding of the document. The None is used when querying\n        the index since the values are not needed. At upsert time, the\n        values are required.\n\n    id : str | None\n        The human-readable identifier of the document. This is generated\n        automatically when creating the PineconeDocument unless it is\n        provided.\n\n    \"\"\"\n\n    metadata: PineconeMetadata\n    values: list[float] | None = None\n    id: str | None = None\n\n    @model_validator(mode=\"after\")\n    def generate_human_readable_id(self) -&gt; \"PineconeDocument\":\n        \"\"\"Generate a human-readable identifier for the document.\"\"\"\n        if self.id is None:\n            meta = self.metadata\n            hr_id = f\"{meta.filename}-{meta.page_number}-{meta.chunk_number}\"\n            self.id = hr_id\n\n        return self\n</code></pre>"},{"location":"api/#whyhow_rbr.rag.PineconeDocument.generate_human_readable_id","title":"<code>generate_human_readable_id()</code>","text":"<p>Generate a human-readable identifier for the document.</p> Source code in <code>whyhow_rbr/rag.py</code> <pre><code>@model_validator(mode=\"after\")\ndef generate_human_readable_id(self) -&gt; \"PineconeDocument\":\n    \"\"\"Generate a human-readable identifier for the document.\"\"\"\n    if self.id is None:\n        meta = self.metadata\n        hr_id = f\"{meta.filename}-{meta.page_number}-{meta.chunk_number}\"\n        self.id = hr_id\n\n    return self\n</code></pre>"},{"location":"api/#whyhow_rbr.rag.PineconeMatch","title":"<code>whyhow_rbr.rag.PineconeMatch</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>The match returned from Pinecone.</p> <p>Attributes:</p> Name Type Description <code>id</code> <code>str</code> <p>The ID of the document.</p> <code>score</code> <code>float</code> <p>The score of the match. Its meaning depends on the metric used for the index.</p> <code>metadata</code> <code>PineconeMetadata</code> <p>The metadata of the document.</p> Source code in <code>whyhow_rbr/rag.py</code> <pre><code>class PineconeMatch(BaseModel, extra=\"ignore\"):\n    \"\"\"The match returned from Pinecone.\n\n    Attributes\n    ----------\n    id : str\n        The ID of the document.\n\n    score : float\n        The score of the match. Its meaning depends on the metric used for\n        the index.\n\n    metadata : PineconeMetadata\n        The metadata of the document.\n\n    \"\"\"\n\n    id: str\n    score: float\n    metadata: PineconeMetadata\n</code></pre>"},{"location":"api/#whyhow_rbr.rag.Input","title":"<code>whyhow_rbr.rag.Input</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Example input for the prompt.</p> <p>Attributes:</p> Name Type Description <code>question</code> <code>str</code> <p>The question to ask.</p> <code>contexts</code> <code>list[str]</code> <p>The contexts to use for answering the question.</p> Source code in <code>whyhow_rbr/rag.py</code> <pre><code>class Input(BaseModel):\n    \"\"\"Example input for the prompt.\n\n    Attributes\n    ----------\n    question : str\n        The question to ask.\n\n    contexts : list[str]\n        The contexts to use for answering the question.\n    \"\"\"\n\n    question: str\n    contexts: list[str]\n</code></pre>"},{"location":"api/#whyhow_rbr.rag.Output","title":"<code>whyhow_rbr.rag.Output</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Example output for the prompt.</p> <p>Attributes:</p> Name Type Description <code>answer</code> <code>str</code> <p>The answer to the question.</p> <code>contexts</code> <code>list[int]</code> <p>The indices of the contexts that were used to answer the question.</p> Source code in <code>whyhow_rbr/rag.py</code> <pre><code>class Output(BaseModel):\n    \"\"\"Example output for the prompt.\n\n    Attributes\n    ----------\n    answer : str\n        The answer to the question.\n\n    contexts : list[int]\n        The indices of the contexts that were used to answer the question.\n    \"\"\"\n\n    answer: str\n    contexts: list[int]\n</code></pre>"},{"location":"api/#whyhow_rbr.rag.Client.query","title":"<code>whyhow_rbr.rag.Client.query(question, index, namespace, rules=None, top_k=5, chat_model='gpt-4-1106-preview', chat_temperature=0.0, chat_max_tokens=1000, chat_seed=2, embedding_model='text-embedding-3-small', process_rules_separately=False, keyword_trigger=False)</code>","text":"<p>Query the index.</p> <p>Parameters:</p> Name Type Description Default <code>question</code> <code>str</code> <p>The question to ask.</p> required <code>index</code> <code>Index</code> <p>The index to query.</p> required <code>namespace</code> <code>str</code> <p>The namespace within the index to use.</p> required <code>rules</code> <code>list[Rule] | None</code> <p>The rules to use for filtering the documents.</p> <code>None</code> <code>top_k</code> <code>int</code> <p>The number of matches to return per rule.</p> <code>5</code> <code>chat_model</code> <code>str</code> <p>The OpenAI chat model to use.</p> <code>'gpt-4-1106-preview'</code> <code>chat_temperature</code> <code>float</code> <p>The temperature for the chat model.</p> <code>0.0</code> <code>chat_max_tokens</code> <code>int</code> <p>The maximum number of tokens for the chat model.</p> <code>1000</code> <code>chat_seed</code> <code>int</code> <p>The seed for the chat model.</p> <code>2</code> <code>embedding_model</code> <code>str</code> <p>The OpenAI embedding model to use.</p> <code>'text-embedding-3-small'</code> <code>process_rules_separately</code> <code>bool</code> <p>Whether to process each rule individually and combine the results at the end. When set to True, each rule will be run independently, ensuring that every rule returns results. When set to False (default), all rules will be run as one joined query, potentially allowing one rule to dominate the others. Default is False.</p> <code>False</code> <code>keyword_trigger</code> <code>bool</code> <p>Whether to trigger rules based on keyword matches in the question. Default is False.</p> <code>False</code> <p>Returns:</p> Type Description <code>QueryReturnType</code> <p>Dictionary with keys \"answer\", \"matches\", and \"used_contexts\". The \"answer\" is the answer to the question. The \"matches\" are the \"top_k\" matches from the index. The \"used_contexts\" are the indices of the matches that were actually used to answer the question.</p> <p>Raises:</p> Type Description <code>OpenAIException</code> <p>If there is an error with the OpenAI API. Some possible reasons include the chat model not finishing or the response not being valid JSON.</p> Source code in <code>whyhow_rbr/rag.py</code> <pre><code>def query(\n    self,\n    question: str,\n    index: Index,\n    namespace: str,\n    rules: list[Rule] | None = None,\n    top_k: int = 5,\n    chat_model: str = \"gpt-4-1106-preview\",\n    chat_temperature: float = 0.0,\n    chat_max_tokens: int = 1000,\n    chat_seed: int = 2,\n    embedding_model: str = \"text-embedding-3-small\",\n    process_rules_separately: bool = False,\n    keyword_trigger: bool = False,\n) -&gt; QueryReturnType:\n    \"\"\"Query the index.\n\n    Parameters\n    ----------\n    question : str\n        The question to ask.\n\n    index : Index\n        The index to query.\n\n    namespace : str\n        The namespace within the index to use.\n\n    rules : list[Rule] | None\n        The rules to use for filtering the documents.\n\n    top_k : int\n        The number of matches to return per rule.\n\n    chat_model : str\n        The OpenAI chat model to use.\n\n    chat_temperature : float\n        The temperature for the chat model.\n\n    chat_max_tokens : int\n        The maximum number of tokens for the chat model.\n\n    chat_seed : int\n        The seed for the chat model.\n\n    embedding_model : str\n        The OpenAI embedding model to use.\n\n    process_rules_separately : bool, optional\n        Whether to process each rule individually and combine the results at the end.\n        When set to True, each rule will be run independently, ensuring that every rule\n        returns results. When set to False (default), all rules will be run as one joined\n        query, potentially allowing one rule to dominate the others.\n        Default is False.\n\n    keyword_trigger : bool, optional\n        Whether to trigger rules based on keyword matches in the question.\n        Default is False.\n\n    Returns\n    -------\n    QueryReturnType\n        Dictionary with keys \"answer\", \"matches\", and \"used_contexts\".\n        The \"answer\" is the answer to the question.\n        The \"matches\" are the \"top_k\" matches from the index.\n        The \"used_contexts\" are the indices of the matches\n        that were actually used to answer the question.\n\n    Raises\n    ------\n    OpenAIException\n        If there is an error with the OpenAI API. Some possible reasons\n        include the chat model not finishing or the response not being\n        valid JSON.\n    \"\"\"\n    logger.info(f\"Raw rules: {rules}\")\n\n    if rules is None:\n        rules = []\n\n    if keyword_trigger:\n        triggered_rules = []\n        clean_question = self.clean_text(question).split(\" \")\n\n        for rule in rules:\n            if rule.keywords:\n                clean_keywords = [\n                    self.clean_text(keyword) for keyword in rule.keywords\n                ]\n\n                if bool(set(clean_keywords) &amp; set(clean_question)):\n                    triggered_rules.append(rule)\n\n        rules = triggered_rules\n\n    rule_filters = [rule.to_filter() for rule in rules if rule is not None]\n\n    question_embedding = generate_embeddings(\n        openai_api_key=self.openai_client.api_key,\n        chunks=[question],\n        model=embedding_model,\n    )[0]\n\n    matches = (\n        []\n    )  # Initialize matches outside the loop to collect matches from all queries\n    match_texts = []\n\n    # Check if there are any rule filters, and if not, proceed with a default query\n    if not rule_filters:\n        # Perform a default query\n        query_response = index.query(\n            namespace=namespace,\n            top_k=top_k,\n            vector=question_embedding,\n            filter=None,  # No specific filter, or you can define a default filter as per your application's logic\n            include_metadata=True,\n        )\n        matches = [\n            PineconeMatch(**m.to_dict()) for m in query_response[\"matches\"]\n        ]\n        match_texts = [m.metadata.text for m in matches]\n\n    else:\n\n        if process_rules_separately:\n            for rule_filter in rule_filters:\n                if rule_filter:\n                    query_response = index.query(\n                        namespace=namespace,\n                        top_k=top_k,\n                        vector=question_embedding,\n                        filter=rule_filter,\n                        include_metadata=True,\n                    )\n                    matches.extend(\n                        [\n                            PineconeMatch(**m.to_dict())\n                            for m in query_response[\"matches\"]\n                        ]\n                    )\n                    match_texts += [m.metadata.text for m in matches]\n            match_texts = list(\n                set(match_texts)\n            )  # Ensure unique match texts\n        else:\n            if rule_filters:\n                combined_filters = []\n                for rule_filter in rule_filters:\n                    if rule_filter:\n                        combined_filters.append(rule_filter)\n\n                rule_filter = (\n                    {\"$or\": combined_filters} if combined_filters else None\n                )\n            else:\n                rule_filter = None  # Fallback to a default query when no rules are provided or valid\n\n            if rule_filter is not None:\n                query_response = index.query(\n                    namespace=namespace,\n                    top_k=top_k,\n                    vector=question_embedding,\n                    filter=rule_filter,\n                    include_metadata=True,\n                )\n                matches = [\n                    PineconeMatch(**m.to_dict())\n                    for m in query_response[\"matches\"]\n                ]\n                match_texts = [m.metadata.text for m in matches]\n\n    # Proceed to create prompt, send it to OpenAI, and handle the response\n    prompt = self.create_prompt(question, match_texts)\n    response = self.openai_client.chat.completions.create(\n        model=chat_model,\n        seed=chat_seed,\n        temperature=chat_temperature,\n        messages=[{\"role\": \"user\", \"content\": prompt}],\n        max_tokens=chat_max_tokens,\n    )\n\n    output = self.process_response(response)\n\n    return_dict: QueryReturnType = {\n        \"answer\": output.answer,\n        \"matches\": [m.model_dump() for m in matches],\n        \"used_contexts\": output.contexts,\n    }\n\n    return return_dict\n</code></pre>"},{"location":"installation/","title":"Installation","text":"<p>To install the Rule-based Retrieval package, follow these steps:</p>"},{"location":"installation/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.10 or higher</li> <li>OpenAI API key</li> <li>Pinecone API key</li> </ul>"},{"location":"installation/#install-from-github","title":"Install from GitHub","text":"<p>Clone the repository:</p> <pre><code>git clone git@github.com:whyhow-ai/rule-based-retrieval.git\ncd rule-based-retrieval\n</code></pre> <p>Install the packages:</p> <pre><code>pip install .\n</code></pre> <p>Set the required environment variables:</p> <pre><code>export OPENAI_API_KEY=&lt;your open ai api key&gt;\nexport PINECONE_API_KEY=&lt;your pinecone api key&gt;\n</code></pre>"},{"location":"installation/#developer-installation","title":"Developer Installation","text":"<p>For a developer installation, use an editable install and include the development dependencies:</p> <pre><code>pip install -e .[dev]\n</code></pre> <p>For ZSH:</p> <pre><code>pip install -e \".[dev]\"\n</code></pre>"},{"location":"installation/#install-documentation-dependencies","title":"Install Documentation Dependencies","text":"<p>To build and serve the documentation locally, install the documentation dependencies:</p> <pre><code>pip install -e .[docs]\n</code></pre> <p>For ZSH:</p> <pre><code>pip install -e \".[docs]\"\n</code></pre> <p>Then, use mkdocs to serve the documentation:</p> <pre><code>mkdocs serve\n</code></pre> <p>Navigate to http://127.0.0.1:8000/ in your browser to view the documentation.</p>"},{"location":"installation/#troubleshooting","title":"Troubleshooting","text":"<p>If you encounter any issues during installation, please check the following:</p> <ul> <li>Ensure that you have Python 3.10 or higher installed. You can check your Python version by running <code>python --version</code> in your terminal.</li> <li>Make sure that you have correctly set the <code>OPENAI_API_KEY</code> and <code>PINECONE_API_KEY</code> environment variables with your respective API keys.</li> <li>If you are installing from GitHub, ensure that you have cloned the repository correctly and are in the right directory.</li> <li>If you are using a virtual environment, make sure that it is activated before running the installation commands.</li> <li>If you still face problems, please open an issue on the GitHub repository with detailed information about the error and your environment setup.</li> </ul>"},{"location":"milvus/","title":"Tutorial of Rule-based Retrieval through Milvus","text":"<p>The <code>whyhow_rbr</code> package helps create customized RAG pipelines. It is built on top of the following technologies (and their respective Python SDKs)</p> <ul> <li>OpenAI - text generation</li> <li>Milvus - vector database</li> </ul>"},{"location":"milvus/#initialization","title":"Initialization","text":"<p>Please import some essential package <pre><code>from pymilvus import DataType\n\nfrom src.whyhow_rbr.rag_milvus import ClientMilvus\n</code></pre></p>"},{"location":"milvus/#client","title":"Client","text":"<p>The central object is a <code>ClientMilvus</code>. It manages all necessary resources and provides a simple interface for all the RAG related tasks.</p> <p>First of all, to instantiate it one needs to provide the following credentials:</p> <ul> <li><code>OPENAI_API_KEY</code></li> <li><code>Milvus_URI</code></li> <li><code>Milvus_API_TOKEN</code></li> </ul> <p>Initialize the ClientMilvus like this:</p> <pre><code># Set up your Milvus Cloud information\nYOUR_MILVUS_CLOUD_END_POINT=\"YOUR_MILVUS_CLOUD_END_POINT\"\nYOUR_MILVUS_CLOUD_TOKEN=\"YOUR_MILVUS_CLOUD_TOKEN\"\n\n# Initialize the ClientMilvus\nmilvus_client = ClientMilvus(\n    milvus_uri=YOUR_MILVUS_CLOUD_END_POINT,\n    milvus_token=YOUR_MILVUS_CLOUD_TOKEN\n)\n</code></pre>"},{"location":"milvus/#vector-database-operations","title":"Vector database operations","text":"<p>This tutorial <code>whyhow_rbr</code> uses Milvus for everything related to vector databses.</p>"},{"location":"milvus/#defining-necessary-variables","title":"Defining necessary variables","text":"<pre><code># Define collection name\nCOLLECTION_NAME=\"YOUR_COLLECTION_NAME\" # take your own collection name\n\n# Define vector dimension size\nDIMENSION=1536 # decide by the model you use\n</code></pre>"},{"location":"milvus/#add-schema","title":"Add schema","text":"<p>Before inserting any data into Milvus database, we need to first define the data field, which is called schema in here. Through create object <code>CollectionSchema</code> and add data field through <code>addd_field()</code>, we can control our data type and their characteristics. This step is required.</p> <p><pre><code>schema = milvus_client.create_schema(auto_id=True) # Enable id matching\n\nschema = milvus_client.add_field(schema=schema, field_name=\"id\", datatype=DataType.INT64, is_primary=True)\nschema = milvus_client.add_field(schema=schema, field_name=\"embedding\", datatype=DataType.FLOAT_VECTOR, dim=DIMENSION)\n</code></pre> We only defined <code>id</code> and <code>embedding</code> here because we need to define a primary field for each collection. For embedding, we need to define the dimension. We allow <code>enable_dynamic_field</code> which support auto adding schema, but we still encourage you to add schema by yourself. This method is a thin wrapper around the official Milvus implementation (official docs)</p>"},{"location":"milvus/#creating-an-index","title":"Creating an index","text":"<p>For each schema, it is better to have an index so that the querying will be much more efficient. To create an index, we first need an index_params and later add more index data on this <code>IndexParams</code> object. <pre><code># Start to indexing data field\nindex_params = milvus_client.prepare_index_params()\nindex_params = milvus_client.add_index(\n    index_params=index_params,  # pass in index_params object\n    field_name=\"embedding\",\n    index_type=\"AUTOINDEX\",  # use autoindex instead of other complex indexing method\n    metric_type=\"COSINE\",  # L2, COSINE, or IP\n)\n</code></pre> This method  is a thin wrapper around the official Milvus implementation (official docs).</p>"},{"location":"milvus/#create-collection","title":"Create Collection","text":"<p>After defining all the data field and indexing them, we now need to create our database collection so that we can access our data quick and precise. What's need to be mentioned is that we initialized the <code>enable_dynamic_field</code> to be true so that you can upload any data freely. The cost is the data querying might be inefficient. <pre><code># Create Collection\nmilvus_client.create_collection(\n    collection_name=COLLECTION_NAME,\n    schema=schema,\n    index_params=index_params\n)\n</code></pre></p>"},{"location":"milvus/#uploading-documents","title":"Uploading documents","text":"<p>After creating a collection, we are ready to populate it with documents. In <code>whyhow_rbr</code> this is done using the <code>upload_documents</code> method of the <code>MilvusClient</code>. It performs the following steps under the hood:</p> <ul> <li>Preprocessing: Reading and splitting the provided PDF files into chunks</li> <li>Embedding: Embedding all the chunks using an OpenAI model</li> <li>Inserting: Uploading both the embeddings and the metadata to a Milvus collection</li> </ul> <p>See below an example of how to use it.</p> <pre><code># get pdfs\npdfs = [\"harry-potter.pdf\", \"game-of-thrones.pdf\"] # replace to your pdfs path\n\n# Uploading the PDF document\nmilvus_client.upload_documents(\n    collection_name=COLLECTION_NAME,\n    documents=pdfs\n)\n</code></pre>"},{"location":"milvus/#question-answering","title":"Question answering","text":"<p>Now we can finally move to retrieval augmented generation.</p> <p>In <code>whyhow_rbr</code> with Milvus, it can be done via the <code>search</code> method.</p> <ol> <li>Simple example:</li> </ol> <pre><code># Search data and implement RAG!\nres = milvus_client.search(\n    question='What food does Harry Potter like to eat?',\n    collection_name=COLLECTION_NAME,\n    anns_field='embedding',\n    output_fields='text'\n)\nprint(res['answer'])\nprint(res['matches'])\n</code></pre> <p>The <code>result</code> is a dictionary that has the following keys</p> <ul> <li><code>answer</code> - the the answer to the question</li> <li><code>matches</code> - the <code>limit</code> most relevant documents from the index</li> </ul> <p>Note that the number of matches will be in general equal to <code>limit</code> which can be specified as a parameter.</p>"},{"location":"milvus/#clean-up","title":"Clean up","text":"<p>At last, after implemented all the instructuons, you can clean up the database by calling <code>drop_collection()</code>. <pre><code># Clean up\nmilvus_client.drop_collection(\n    collection_name=COLLECTION_NAME\n)\n</code></pre></p>"},{"location":"milvus/#rules","title":"Rules","text":"<p>In the previous example, every single document in our index was considered. However, sometimes it might be beneficial to only retrieve documents satisfying some predefined conditions (e.g. <code>filename=harry-potter.pdf</code>). In <code>whyhow_rbr</code> through Milvus, this can be done via adjusting searching parameters.</p> <p>A rule can control the following metadata attributes</p> <ul> <li><code>filename</code> - name of the file</li> <li><code>page_numbers</code> - list of integers corresponding to page numbers (0 indexing)</li> <li><code>id</code> - unique identifier of a chunk (this is the most \"extreme\" filter)</li> <li>Other rules base on Boolean Expressions</li> </ul> <p>Rules Example:</p> <pre><code># RULES(search on book harry-potter on page 8):\nPARTITION_NAME='harry-potter' # search on books\npage_number='page_number == 8'\n\n# first create a partitions to store the book and later search on this specific partition:\nmilvus_client.crate_partition(\n    collection_name=COLLECTION_NAME,\n    partition_name=PARTITION_NAME # separate base on your pdfs type\n)\n\n# search with rules\nres = milvus_client.search(\n    question='Tell me about the greedy method',\n    collection_name=COLLECTION_NAME,\n    partition_names=PARTITION_NAME,\n    filter=page_number, # append any rules follow the Boolean Expression Rule\n    anns_field='embedding',\n    output_fields='text'\n)\nprint(res['answer'])\nprint(res['matches'])\n</code></pre> <p>In this example, we first create a partition that store harry-potter related pdfs, and through searching within this partition, we can get the most direct information.  Also, we apply page number as a filter to specify the exact page we wish to search on. Remember, the filer parameter need to follow the boolean rule.</p> <p>That's all for the Milvus implementation of Rule-based Retrieval.</p>"},{"location":"pinecone/","title":"How to do Rule-based Retrieval through Pinecone","text":"<p>Here is the brief introduction of the main functions of Rule-based Retrieval. For more specific tutorial of Pinecone, you can find it here.</p>"},{"location":"pinecone/#set-up-the-environment","title":"Set up the environment","text":"<pre><code>export OPENAI_API_KEY=&lt;your open ai api key&gt;\nexport PINECONE_API_KEY=&lt;your pinecone api key&gt;\n</code></pre>"},{"location":"pinecone/#create-index-upload","title":"Create index &amp; upload","text":"<pre><code>from whyhow_rbr import Client\n\n# Configure parameters\nindex_name = \"whyhow-demo\"\nnamespace = \"demo\"\npdfs = [\"harry_potter_book_1.pdf\"]\n\n# Initialize client\nclient = Client()\n\n# Create index\nindex = client.get_index(index_name)\n\n# Upload, split, chunk, and vectorize documents in Pinecone\nclient.upload_documents(index=index, documents=pdfs, namespace=namespace)\n</code></pre>"},{"location":"pinecone/#query-with-rules","title":"Query with rules","text":"<pre><code>from whyhow_rbr import Client, Rule\n\n# Configure query parameters\nindex_name = \"whyhow-demo\"\nnamespace = \"demo\"\nquestion = \"What does Harry wear?\"\ntop_k = 5\n\n# Initialize client\nclient = Client()\n\n# Create rules\nrules = [\n    Rule(\n        filename=\"harry_potter_book_1.pdf\",\n        page_numbers=[21, 22, 23]\n    ),\n    Rule(\n        filename=\"harry_potter_book_1.pdf\",\n        page_numbers=[151, 152, 153, 154]\n    )\n]\n\n# Run query\nresult = client.query(\n    question=question,\n    index=index,\n    namespace=namespace,\n    rules=rules,\n    top_k=top_k,\n)\n\nanswer = result[\"answer\"]\nused_contexts = [\n    result[\"matches\"][i][\"metadata\"][\"text\"] for i in result[\"used_contexts\"]\n]\nprint(f\"Answer: {answer}\")\nprint(\n    f\"The model used {len(used_contexts)} chunk(s) from the DB to answer the question\"\n)\n</code></pre>"},{"location":"pinecone/#query-with-keywords","title":"Query with keywords","text":"<pre><code>from whyhow_rbr import Client, Rule\n\nclient = Client()\n\nindex = client.get_index(\"amazing-index\")\nnamespace = \"books\"\n\nquestion = \"What does Harry Potter like to eat?\"\n\nrule = Rule(\n    filename=\"harry-potter.pdf\",\n    keywords=[\"food\", \"favorite\", \"likes to eat\"]\n)\n\nresult = client.query(\n    question=question,\n    index=index,\n    namespace=namespace,\n    rules=[rule],\n    keyword_trigger=True\n)\n\nprint(result[\"answer\"])\nprint(result[\"matches\"])\nprint(result[\"used_contexts\"])\n</code></pre>"},{"location":"pinecone/#query-each-rule-separately","title":"Query each rule separately","text":"<pre><code>from whyhow_rbr import Client, Rule\n\nclient = Client()\n\nindex = client.get_index(\"amazing-index\")\nnamespace = \"books\"\n\nquestion = \"What is Harry Potter's favorite food?\"\n\nrule_1 = Rule(\n    filename=\"harry-potter.pdf\",\n    page_numbers=[120, 121, 150]\n)\n\nrule_2 = Rule(\n    filename=\"harry-potter-volume-2.pdf\",\n    page_numbers=[80, 81, 82]\n)\n\nresult = client.query(\n    question=question,\n    index=index,\n    namespace=namespace,\n    rules=[rule_1, rule_2],\n    process_rules_separately=True\n)\n\nprint(result[\"answer\"])\nprint(result[\"matches\"])\nprint(result[\"used_contexts\"])\n</code></pre>"},{"location":"tutorial/","title":"Tutorial","text":"<p>The <code>whyhow_rbr</code> package helps create customized RAG pipelines. It is built on top of the following technologies (and their respective Python SDKs)</p> <ul> <li>OpenAI - text generation</li> <li>Pinecone - vector database</li> </ul>"},{"location":"tutorial/#client","title":"Client","text":"<p>The central object is a <code>Client</code>. It manages all necessary resources and provides a simple interface for all the RAG related tasks.</p> <p>First of all, to instantiate it one needs to provide the following API keys:</p> <ul> <li><code>OPENAI_API_KEY</code></li> <li><code>PINECONE_API_KEY</code></li> </ul> <p>One can either define the corresponding environment variables</p> <pre><code>export OPENAI_API_KEY=...\nexport PINECONE_API_KEY...\n</code></pre> <p>and then instantiate the client without any arguments.</p> getting_started.py<pre><code>from whyhow_rbr import Client\n\nclient = Client()\n</code></pre> <pre><code>python getting_started.py\n</code></pre> <p>An alternative approach is to manually pass the keys when the client is being constructed</p> getting_started.py<pre><code>from whyhow_rbr import Client\n\nclient = Client(\n    openai_api_key=\"...\",\n    pinecone_api_key=\"...\"\n\n)\n</code></pre> <pre><code>python getting_started.py\n</code></pre>"},{"location":"tutorial/#vector-database-operations","title":"Vector database operations","text":"<p><code>whyhow_rbr</code> uses Pinecone for everything related to vector databses.</p>"},{"location":"tutorial/#creating-an-index","title":"Creating an index","text":"<p>If you don't have a Pinecone index yet, you can create it using the <code>create_index</code> method of the <code>Client</code>. This method is a thin wrapper around the Pinecone SDK (official docs).</p> <p>First of all, you need to provide a specification. There are 2 types</p> <ul> <li>Serverless</li> <li>Pod-based</li> </ul>"},{"location":"tutorial/#serverless","title":"Serverless","text":"<p>To create a serverless index you can use</p> <pre><code># Code above omitted \ud83d\udc46\n\nfrom pinecone import ServerlessSpec\n\nspec = ServerlessSpec(\n    cloud=\"aws\",\n    region=\"us-west-2\"\n)\n\nindex = client.create_index(\n    name=\"great-index\",  # the only required argument\n    dimension=1536\n    metric=\"cosine\",\n    spec=spec\n)\n</code></pre> Full code <pre><code>from pinecone import ServerlessSpec\n\nfrom whyhow_rbr import Client\n\nclient = Client()\n\nspec = ServerlessSpec(\n    cloud=\"aws\",\n    region=\"us-west-2\"\n)\n\nindex = client.create_index(\n    name=\"great-index\",  # the only required argument\n    dimension=1536\n    metric=\"cosine\",\n    spec=spec\n)\n</code></pre>"},{"location":"tutorial/#pod-based","title":"Pod-based","text":"<p>To create a pod-based index you can use</p> <pre><code># Code above omitted \ud83d\udc46\n\nfrom pinecone import PodSpec\n\nspec = PodSpec(\n    environment=\"gcp-starter\"\n)\n\nindex = client.create_index(\n    name=\"amazing-index\",  # the only required argument\n    dimension=1536\n    metric=\"cosine\",\n    spec=spec\n)\n</code></pre> Full code <pre><code>from pinecone import PodSpec\n\nfrom whyhow_rbr import Client\n\nclient = Client()\n\nspec = PodSpec(\n    environment=\"gcp-starter\"\n)\n\nindex = client.create_index(\n    name=\"amazing-index\",  # the only required argument\n    dimension=1536\n    metric=\"cosine\",\n    spec=spec\n)\n</code></pre> <p>Info</p> <p>For detailed information on what all of the parameters mean please refer to (Pinecone) Understanding indices</p>"},{"location":"tutorial/#getting-an-existing-index","title":"Getting an existing index","text":"<p>If your exists already, you can use the <code>get_index</code> method to get it.</p> <pre><code># Code above omitted \ud83d\udc46\n\nindex = client.get_index(\"amazing-index\")\n</code></pre> Full code <pre><code>from pinecone import PodSpec\n\nfrom whyhow_rbr import Client\n\nclient = Client()\n\nindex = client.get_index(\"amazing-index\")\n</code></pre>"},{"location":"tutorial/#index-operations","title":"Index operations","text":"<p>Both <code>create_index</code> and <code>get_index</code> return an instance of <code>pinecone.Index</code>. It offers multiple convenience methods. See below a few examples.</p>"},{"location":"tutorial/#describe_index_stats","title":"<code>describe_index_stats</code>","text":"<p>Shows useful information about the index.</p> <pre><code>index.describe_index_stats()\n</code></pre> <p>Example output:</p> <pre><code>{'dimension': 1536,\n 'index_fullness': 0.00448,\n 'namespaces': {'A': {'vector_count': 11},\n                'B': {'vector_count': 11},\n                'C': {'vector_count': 62},\n                'D': {'vector_count': 82},\n                'E': {'vector_count': 282}},\n 'total_vector_count': 448}\n</code></pre>"},{"location":"tutorial/#fetch","title":"<code>fetch</code>","text":"<p>Fetch (Pinecone docs)</p>"},{"location":"tutorial/#upsert","title":"<code>upsert</code>","text":"<p>Upsert (Pinecone docs)</p>"},{"location":"tutorial/#query","title":"<code>query</code>","text":"<p>Query (Pinecone docs)</p>"},{"location":"tutorial/#delete","title":"<code>delete</code>","text":"<p>Delete (Pinecone docs)</p>"},{"location":"tutorial/#update","title":"<code>update</code>","text":"<p>Update (Pinecone docs)</p>"},{"location":"tutorial/#uploading-documents","title":"Uploading documents","text":"<p>After creating an index, we are ready to populate it with documents. In <code>whyhow_rbr</code> this is done using the <code>upload_documents</code> method of the <code>Client</code>. It performs the following steps under the hood:</p> <ul> <li>Preprocessing: Reading and splitting the provided PDF files into chunks</li> <li>Embedding: Embedding all the chunks using an OpenAI model</li> <li>Upserting: Uploading both the embeddings and the metadata to a Pinecone index</li> </ul> <p>See below an example of how to use it.</p> <pre><code># Code above omitted \ud83d\udc46\n\nnamespace = \"books\"\npdfs = [\"harry-potter.pdf\", \"game-of-thrones.pdf\"]\n\nclient.upload_documents(\n    index=index,\n    documents=pdfs,\n    namespace=namespace\n)\n</code></pre> Full code <pre><code>from whyhow_rbr import Client\n\nclient = Client()\n\nindex = client.get_index(\"amazing-index\")\n\nnamespace = \"books\"\npdfs = [\"harry-potter.pdf\", \"game-of-thrones.pdf\"]\n\nclient.upload_documents(\n    index=index,\n    documents=pdfs,\n    namespace=namespace\n)\n</code></pre> <p>Warning</p> <p>The above example assumes you have two PDFs on your disk.</p> <ul> <li><code>harry-potter.pdf</code></li> <li><code>game-of-thrones.pdf</code></li> </ul> <p>However, feel free to provide different documents.</p> <p>Info</p> <p>The <code>upload_documents</code> method does not return anything. If you want to get some information about what is going on you can activate logging.</p> <p><pre><code>import logging\n\nlogging.basicConfig(\n    level=logging.WARNING,\n    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n)\n</code></pre> Note that the above affects the root logger, however, you can also just customize the <code>whyhow_rbr</code> logger.</p> <p>Navigate to upload_documents (API docs) if you want to get more information on the parameters.</p>"},{"location":"tutorial/#index-schema","title":"Index schema","text":"<p>While Pinecone does not require each document in an index to have the same schema all the document uploaded via the <code>upload_documents</code> will have a fixed schema. This schema is defined in PineconeDocument (API docs). This is done in order to have a predictable set of attributes that can be used to perform advanced filtering (via rules).</p>"},{"location":"tutorial/#question-answering","title":"Question answering","text":"<p>In previous sections we discussed how to to create an index and populate it with documents. Now we can finally move to retrieval augmented generation.</p> <p>In <code>whyhow_rbr</code>, it can be done via the <code>query</code> method.</p> <ol> <li>Simple example:</li> </ol> <pre><code>from whyhow_rbr import Client, Rule\n\nclient = Client()\n\nindex = client.get_index(\"amazing-index\")\nnamespace = \"books\"\n\nquestion = \"What is Harry Potter's favorite food?\"\n\nrule = Rule(\n    filename=\"harry-potter.pdf\",\n    page_numbers=[120, 121, 150]\n)\n\nresult = client.query(\n    question=question,\n    index=index,\n    namespace=namespace,\n    rules=[rule]\n)\n\nprint(result[\"answer\"])\nprint(result[\"matches\"])\nprint(result[\"used_contexts\"])\n</code></pre> <p>The <code>result</code> is a dictionary that has the following three keys</p> <ul> <li><code>answer</code> - the the answer to the question</li> <li><code>matches</code> - the <code>top_k</code> most relevant documents from the index</li> <li><code>used_contexts</code> - the matches (or more precisely just the texts/contexts) that   the LLM used to answer the question.</li> </ul> <pre><code>print(result[\"answer\"])\n</code></pre> <pre><code>'Treacle tart'\n</code></pre> <pre><code>print(result[\"matches\"])\n</code></pre> <pre><code>[{'id': 'harry-potter.pdf-120-5',\n  'metadata': {'chunk_number': 5,\n               'filename': 'harry-potter.pdf',\n               'page_number': 120,\n               'text': 'Harry loves the treacle tart.'\n               'uuid': '86314e32-7d88-475c-b950-f8c156ebf259'},\n  'score': 0.826438308},\n {'id': 'game-of-thrones.pdf-75-1',\n  'metadata': {'chunk_number': 1,\n               'filename': 'game-of-thrones.pdf',\n               'page_number': 75,\n               'text': 'Harry Strickland was the head of the exiled House Strickland.'\n                       'He enjoys eating roasted beef.'\n               'uuid': '684a978b-e6e7-45e2-8ba4-5c5019c7c676'},\n  'score': 0.2052352},\n  ...\n  ]\n</code></pre> <p>Note that the number of matches will be in general equal to <code>top_k</code> which can be specified as a parameter. Also, each match has a fixed schema - it is a dump of PineconeMatch (API docs).</p> <pre><code>print(result[\"used_contexts\"])\n</code></pre> <pre><code>[0]\n</code></pre> <p>The OpenAI model only used the context from the 1st match when answering the question.</p> Full code <pre><code>from whyhow_rbr import Client\n\nclient = Client()\n\nindex = client.get_index(\"amazing-index\")\n\nnamespace = \"books\"\n\nquestion = \"What is Harry Potter's favourite food?\"\n\nresult = client.query(\n    question=question\n    index=index,\n    namespace=namespace\n)\n\nprint(result[\"answer\"])\nprint(result[\"matches\"])\nprint(result[\"used_contexts\"])\n</code></pre> <p>Navigate to query(API docs) if you want to get more information on the parameters.</p>"},{"location":"tutorial/#rules","title":"Rules","text":"<p>In the previous example, every single document in our index was considered. However, sometimes it might be beneficial to only retrieve documents satisfying some predefined conditions (e.g. <code>filename=harry-potter.pdf</code>). In <code>whyhow_rbr</code> this can be done via the <code>Rule</code> class.</p> <p>A rule can control the following metadata attributes</p> <ul> <li><code>filename</code> - name of the file</li> <li><code>page_numbers</code> - list of integers corresponding to page numbers (0 indexing)</li> <li><code>uuid</code> - unique identifier of a chunk (this is the most \"extreme\" filter)</li> <li> <p><code>keywords</code> - list of keywords to trigger the rule</p> </li> <li> <p>Keyword example:</p> </li> </ul> <pre><code># Code above omitted \ud83d\udc46\n\nfrom whyhow_rbr import Rule\n\nquestion = \"What is Harry Potter's favourite food?\"\n\nrule = Rule(\n    filename=\"harry-potter.pdf\",\n    page_numbers=[120, 121, 150],\n    keywords=[\"food\", \"favorite\", \"likes to eat\"]\n)\nresult = client.query(\n    question=question\n    index=index,\n    namespace=namespace,\n    rules=[rule],\n    keyword_trigger=True\n)\n</code></pre> <p>In this example, the keyword_trigger parameter is set to True, and the rule includes keywords. Only the rules whose keywords match the words in the question will be applied.</p> Full code <pre><code>from whyhow_rbr import Client, Rule\n\nclient = Client()\n\nindex = client.get_index(\"amazing-index\")\nnamespace = \"books\"\n\nquestion = \"What does Harry Potter like to eat?\"\n\nrule = Rule(\n    filename=\"harry-potter.pdf\",\n    keywords=[\"food\", \"favorite\", \"likes to eat\"]\n)\n\nresult = client.query(\n    question=question,\n    index=index,\n    namespace=namespace,\n    rules=[rule],\n    keyword_trigger=True\n)\n\nprint(result[\"answer\"])\nprint(result[\"matches\"])\nprint(result[\"used_contexts\"])\n</code></pre> <ol> <li>Process rules separately example:</li> </ol> <p>Lastly, you can specify multiple rules at the same time. They will be evaluated using the <code>OR</code> logical operator.</p> <pre><code># Code above omitted \ud83d\udc46\n\nfrom whyhow_rbr import Rule\n\nquestion = \"What is Harry Potter's favorite food?\"\n\nrule_1 = Rule(\n    filename=\"harry-potter.pdf\",\n    page_numbers=[120, 121, 150]\n)\n\nrule_2 = Rule(\n    filename=\"harry-potter-volume-2.pdf\",\n    page_numbers=[80, 81, 82]\n)\n\nresult = client.query(\n    question=question,\n    index=index,\n    namespace=namespace,\n    rules=[rule_1, rule_2],\n    process_rules_separately=True\n)\n</code></pre> <p>In this example, the process_rules_separately parameter is set to True. This means that each rule (rule_1 and rule_2) will be processed independently, ensuring that both rules contribute to the final result set.</p> <p>By default, all rules are run as one joined query, which means that one rule can dominate the others, and given the limit by top_k, a lower priority rule might not return any results. However, by setting process_rules_separately to True, each rule will be processed independently, ensuring that every rule returns results, and the results will be combined at the end.</p> <p>Depending on the number of rules you use in your query, you may return more chunks than your LLM\u2019s context window can handle. Be mindful of your model\u2019s token limits and adjust your top_k and rule count accordingly.</p> Full code <pre><code>from whyhow_rbr import Client, Rule\n\nclient = Client()\n\nindex = client.get_index(\"amazing-index\")\nnamespace = \"books\"\n\nquestion = \"What is Harry Potter's favorite food?\"\n\nrule_1 = Rule(\n    filename=\"harry-potter.pdf\",\n    page_numbers=[120, 121, 150]\n)\n\nrule_2 = Rule(\n    filename=\"harry-potter-volume-2.pdf\",\n    page_numbers=[80, 81, 82]\n)\n\nresult = client.query(\n    question=question,\n    index=index,\n    namespace=namespace,\n    rules=[rule_1, rule_2],\n    process_rules_separately=True\n)\n\nprint(result[\"answer\"])\nprint(result[\"matches\"])\nprint(result[\"used_contexts\"])\n</code></pre> <p>Navigate to Rule (API docs) if you want to get more information on the parameters.</p>"}]}